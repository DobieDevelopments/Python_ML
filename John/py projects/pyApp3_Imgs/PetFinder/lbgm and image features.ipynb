{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import lightgbm as lgb\n",
    "np.random.seed(369)\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following 3 functions have been taken from Ben Hamner's github repository\n",
    "# https://github.com/benhamner/Metrics\n",
    "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = y\n",
    "    rater_b = y_pred\n",
    "    min_rating=None\n",
    "    max_rating=None\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return (1.0 - numerator / denominator)\n",
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        ll = quadratic_weighted_kappa(y, X_p)\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']\n",
    "    \n",
    "def rmse(actual, predicted):\n",
    "    return sqrt(mean_squared_error(actual, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Features ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from keras.applications.densenet import preprocess_input, DenseNet121\n",
    "import tensorflow as tf\n",
    "\n",
    "train_df = pd.read_csv('input/train/train.csv')\n",
    "img_size = 256\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pet_ids = train_df['PetID'].values\n",
    "n_batches = len(pet_ids) // batch_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_to_square(im):\n",
    "    old_size = im.shape[:2] # old_size is in (height, width) format\n",
    "    ratio = float(img_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    # new_size should be in (width, height) format\n",
    "    im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "    delta_w = img_size - new_size[1]\n",
    "    delta_h = img_size - new_size[0]\n",
    "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "    color = [0, 0, 0]\n",
    "    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n",
    "    return new_im\n",
    "\n",
    "def load_image(path, pet_id):\n",
    "    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n",
    "    new_image = resize_to_square(image)\n",
    "    new_image = preprocess_input(new_image)\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "\n",
    "def train_keras():\n",
    "    from keras.models import Model\n",
    "    from keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "\n",
    "    #https://regressionsessionsblog.wordpress.com/2018/06/11/stuck-on-an-issue-making-keras-predictions-in-parallel/\n",
    "    #https://github.com/keras-team/keras/issues/4740\n",
    "    jobs = 6\n",
    "    config = tf.ConfigProto(intra_op_parallelism_threads=jobs, \\\n",
    "                            inter_op_parallelism_threads=jobs, \\\n",
    "                            allow_soft_placement=True, \\\n",
    "                            device_count = {'CPU': jobs})\n",
    "    session = tf.Session(config=config)\n",
    "    K.set_session(session)\n",
    "\n",
    "    print(\"start\")\n",
    "    inp = Input((256,256,3))\n",
    "    backbone = DenseNet121(input_tensor = inp, \n",
    "                            weights=\"input/densenet-keras/DenseNet-BC-121-32-no-top.h5\",\n",
    "                            include_top = False)\n",
    "    x = backbone.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\n",
    "    x = AveragePooling1D(4)(x)\n",
    "    out = Lambda(lambda x: x[:,:,0])(x)\n",
    "\n",
    "    m = Model(inp,out)\n",
    "    print(\"done\")\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"start keras prediction\")\n",
    "\n",
    "    for b in tqdm_notebook(range(n_batches)):\n",
    "        start = b*batch_size\n",
    "        end = (b+1)*batch_size\n",
    "        batch_pets = pet_ids[start:end]\n",
    "        batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            try:\n",
    "                batch_images[i] = load_image(\"input/train/train_images/\", pet_id)\n",
    "            except:\n",
    "                pass\n",
    "        batch_preds = m.predict(batch_images)\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            features[pet_id] = batch_preds[i]\n",
    "    print(\"done keras prediction\")\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "done\n",
      "start keras prediction\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486dd2327a1d45338cf1c6924c13c0a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=921), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-04eb2f141891>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_keras\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-f1a6437cabe2>\u001b[0m in \u001b[0;36mtrain_keras\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mbatch_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpet_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_pets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpet_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_preds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1165\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1167\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32mD:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2617\u001b[0m                 array_vals.append(\n\u001b[0;32m   2618\u001b[0m                     np.asarray(value,\n\u001b[1;32m-> 2619\u001b[1;33m                                dtype=tf.as_dtype(tensor.dtype).as_numpy_dtype))\n\u001b[0m\u001b[0;32m   2620\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2621\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "m = train_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(filename=\"file\", dictionary=pets):\n",
    "    with open(filename + \".json\", 'w') as file:\n",
    "        json.dump(dictionary, file, sort_keys=True, indent=4)\n",
    "    return\n",
    "def load_json(filename=\"file\", dictionary=pets):\n",
    "    with open(filename + \".json\", 'r') as file:\n",
    "        loaded_fule = json.load(file)\n",
    "    return loaded_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json   \n",
    "# serialize model to JSON\n",
    "model_json = m.to_json()\n",
    "print(\"Saved model to disk\")\n",
    "m.save_weights(\"keras_model.h5\")\n",
    "print(\"Saved model weights to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
