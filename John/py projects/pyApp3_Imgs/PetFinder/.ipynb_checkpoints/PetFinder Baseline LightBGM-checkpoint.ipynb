{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import lightgbm as lgb\n",
    "#A fast, distributed, high performance gradient boosting (GBDT, GBRT, GBM or MART) framework \n",
    "#based on decision tree algorithms, used for ranking, classification and many other machine learning tasks.\n",
    "np.random.seed(369)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following 3 functions have been taken from Ben Hamner's github repository\n",
    "# https://github.com/benhamner/Metrics\n",
    "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = y\n",
    "    rater_b = y_pred\n",
    "    min_rating=None\n",
    "    max_rating=None\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return (1.0 - numerator / denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        ll = quadratic_weighted_kappa(y, X_p)\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(actual, predicted):\n",
    "    return sqrt(mean_squared_error(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "(14720, 24)\n",
      "Test\n",
      "(3948, 23)\n",
      "Breeds\n",
      "(307, 3)\n",
      "Colors\n",
      "(7, 2)\n",
      "States\n",
      "(15, 2)\n"
     ]
    }
   ],
   "source": [
    "#Load all CSV data, printing the rows and columns.\n",
    "print('Train')\n",
    "train = pd.read_csv(\"input/train/train.csv\")\n",
    "print(train.shape)\n",
    "\n",
    "print('Test')\n",
    "test = pd.read_csv(\"input/test/test.csv\")\n",
    "print(test.shape)\n",
    "\n",
    "print('Breeds')\n",
    "breeds = pd.read_csv(\"input/breed_labels.csv\")\n",
    "print(breeds.shape)\n",
    "\n",
    "print('Colors')\n",
    "colors = pd.read_csv(\"input/color_labels.csv\")\n",
    "print(colors.shape)\n",
    "\n",
    "print('States')\n",
    "states = pd.read_csv(\"input/state_labels.csv\")\n",
    "print(states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up arrays for the CSV columns that we want\n",
    "target = train['AdoptionSpeed']\n",
    "train_id = train['PetID']\n",
    "test_id = test['PetID']\n",
    "train.drop(['AdoptionSpeed', 'PetID'], axis=1, inplace=True)\n",
    "test.drop(['PetID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup sentiment magnitudes and scores arrays\n",
    "doc_sent_mag = []\n",
    "doc_sent_score = []\n",
    "nf_count = 0\n",
    "for pet in train_id:\n",
    "    try:\n",
    "        with open('input/train/train_sentiment/' + '.json', 'r') as f:\n",
    "            sentiment = json.load(f)\n",
    "        doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "        doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "    except FileNotFoundError:\n",
    "        nf_count += 1\n",
    "        doc_sent_mag.append(-1)\n",
    "        doc_sent_score.append(-1)\n",
    "\n",
    "train.loc[:, 'doc_sent_mag'] = doc_sent_mag\n",
    "train.loc[:, 'doc_sent_score'] = doc_sent_score\n",
    "\n",
    "doc_sent_mag = []\n",
    "doc_sent_score = []\n",
    "nf_count = 0\n",
    "for pet in test_id:\n",
    "    try:\n",
    "        with open('input/test/test_sentiment/' + '.json', 'r') as f:\n",
    "            sentiment = json.load(f)\n",
    "        doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "        doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "    except FileNotFoundError:\n",
    "        nf_count += 1\n",
    "        doc_sent_mag.append(-1)\n",
    "        doc_sent_score.append(-1)\n",
    "\n",
    "test.loc[:, 'doc_sent_mag'] = doc_sent_mag\n",
    "test.loc[:, 'doc_sent_score'] = doc_sent_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (tfidf): (14720, 178520)\n",
      "X (svd): (14720, 200)\n",
      "train: (14720, 24)\n"
     ]
    }
   ],
   "source": [
    "# sci-kit learn TFIDF Vectorizer is used for text feature extraction\n",
    "# https://stackoverflow.com/questions/36800654/how-is-the-tfidfvectorizer-in-scikit-learn-supposed-to-work\n",
    "# Truncated SVD is used to reduce dimensionality\n",
    "train_desc = train.Description.fillna(\"none\").values\n",
    "test_desc = test.Description.fillna(\"none\").values\n",
    "\n",
    "tfv = TfidfVectorizer(min_df=2,  max_features=None,\n",
    "        strip_accents='unicode', analyzer='word', token_pattern=r'(?u)\\b\\w+\\b',\n",
    "        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "        )\n",
    "    \n",
    "# Fit TFIDF\n",
    "tfv.fit(list(train_desc))\n",
    "X =  tfv.transform(train_desc)\n",
    "X_test = tfv.transform(test_desc)\n",
    "print(\"X (tfidf):\", X.shape)\n",
    "\n",
    "svd = TruncatedSVD(n_components=200)\n",
    "svd.fit(X)\n",
    "# print(svd.explained_variance_ratio_.sum())\n",
    "# print(svd.explained_variance_ratio_)\n",
    "X = svd.transform(X)\n",
    "print(\"X (svd):\", X.shape)\n",
    "\n",
    "# X = pd.DataFrame(X, columns=['svd_{}'.format(i) for i in range(120)])\n",
    "# train = pd.concat((train, X), axis=1)\n",
    "# X_test = svd.transform(X_test)\n",
    "# X_test = pd.DataFrame(X_test, columns=['svd_{}'.format(i) for i in range(120)])\n",
    "# test = pd.concat((test, X_test), axis=1)\n",
    "\n",
    "print(\"train:\", train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (tfidf): (14720, 10000)\n",
      "X (svd): (14720, 120)\n",
      "train: (14720, 144)\n"
     ]
    }
   ],
   "source": [
    "## WITHOUT ERROR FIXED\n",
    "train_desc = train.Description.fillna(\"none\").values\n",
    "test_desc = test.Description.fillna(\"none\").values\n",
    "\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=10000,\n",
    "        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "        stop_words = 'english')\n",
    "    \n",
    "# Fit TFIDF\n",
    "tfv.fit(list(train_desc))\n",
    "X =  tfv.transform(train_desc)\n",
    "X_test = tfv.transform(test_desc)\n",
    "print(\"X (tfidf):\", X.shape)\n",
    "\n",
    "svd = TruncatedSVD(n_components=120)\n",
    "svd.fit(X)\n",
    "# print(svd.explained_variance_ratio_.sum())\n",
    "# print(svd.explained_variance_ratio_)\n",
    "X = svd.transform(X)\n",
    "print(\"X (svd):\", X.shape)\n",
    "\n",
    "X = pd.DataFrame(X, columns=['svd_{}'.format(i) for i in range(120)])\n",
    "train = pd.concat((train, X), axis=1)\n",
    "X_test = svd.transform(X_test)\n",
    "X_test = pd.DataFrame(X_test, columns=['svd_{}'.format(i) for i in range(120)])\n",
    "test = pd.concat((test, X_test), axis=1)\n",
    "\n",
    "print(\"train:\", train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train - not found:  14720\n",
      "train - not labelled:  0\n",
      "test - not found:  3948\n"
     ]
    }
   ],
   "source": [
    "#load metadata\n",
    "vertex_xs = []\n",
    "vertex_ys = []\n",
    "bounding_confidences = []\n",
    "bounding_importance_fracs = []\n",
    "dominant_blues = []\n",
    "dominant_greens = []\n",
    "dominant_reds = []\n",
    "dominant_pixel_fracs = []\n",
    "dominant_scores = []\n",
    "label_descriptions = []\n",
    "label_scores = []\n",
    "nf_count = 0 #not found\n",
    "nl_count = 0 #not labelled\n",
    "for pet in train_id:\n",
    "    try:\n",
    "        with open('../input/train/train_metadata/' + '-1.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "        vertex_xs.append(vertex_x)\n",
    "        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "        vertex_ys.append(vertex_y)\n",
    "        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "        bounding_confidences.append(bounding_confidence)\n",
    "        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "        bounding_importance_fracs.append(bounding_importance_frac)\n",
    "        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n",
    "        dominant_blues.append(dominant_blue)\n",
    "        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n",
    "        dominant_greens.append(dominant_green)\n",
    "        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n",
    "        dominant_reds.append(dominant_red)\n",
    "        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "        dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "        dominant_scores.append(dominant_score)\n",
    "        if data.get('labelAnnotations'):\n",
    "            label_description = data['labelAnnotations'][0]['description']\n",
    "            label_descriptions.append(label_description)\n",
    "            label_score = data['labelAnnotations'][0]['score']\n",
    "            label_scores.append(label_score)\n",
    "        else:\n",
    "            nl_count += 1\n",
    "            label_descriptions.append('nothing')\n",
    "            label_scores.append(-1)\n",
    "    except FileNotFoundError:\n",
    "        nf_count += 1\n",
    "        vertex_xs.append(-1)\n",
    "        vertex_ys.append(-1)\n",
    "        bounding_confidences.append(-1)\n",
    "        bounding_importance_fracs.append(-1)\n",
    "        dominant_blues.append(-1)\n",
    "        dominant_greens.append(-1)\n",
    "        dominant_reds.append(-1)\n",
    "        dominant_pixel_fracs.append(-1)\n",
    "        dominant_scores.append(-1)\n",
    "        label_descriptions.append('nothing')\n",
    "        label_scores.append(-1)\n",
    "\n",
    "print('train - not found: ', nf_count)\n",
    "print('train - not labelled: ', nl_count)\n",
    "train.loc[:, 'vertex_x'] = vertex_xs\n",
    "train.loc[:, 'vertex_y'] = vertex_ys\n",
    "train.loc[:, 'bounding_confidence'] = bounding_confidences\n",
    "train.loc[:, 'bounding_importance'] = bounding_importance_fracs\n",
    "train.loc[:, 'dominant_blue'] = dominant_blues\n",
    "train.loc[:, 'dominant_green'] = dominant_greens\n",
    "train.loc[:, 'dominant_red'] = dominant_reds\n",
    "train.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "train.loc[:, 'dominant_score'] = dominant_scores\n",
    "train.loc[:, 'label_description'] = label_descriptions\n",
    "train.loc[:, 'label_score'] = label_scores\n",
    "\n",
    "\n",
    "vertex_xs = []\n",
    "vertex_ys = []\n",
    "bounding_confidences = []\n",
    "bounding_importance_fracs = []\n",
    "dominant_blues = []\n",
    "dominant_greens = []\n",
    "dominant_reds = []\n",
    "dominant_pixel_fracs = []\n",
    "dominant_scores = []\n",
    "label_descriptions = []\n",
    "label_scores = []\n",
    "nf_count = 0\n",
    "nl_count = 0\n",
    "for pet in test_id:\n",
    "    try:\n",
    "        with open('../input/test_metadata/' + '-1.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "        vertex_xs.append(vertex_x)\n",
    "        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "        vertex_ys.append(vertex_y)\n",
    "        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "        bounding_confidences.append(bounding_confidence)\n",
    "        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "        bounding_importance_fracs.append(bounding_importance_frac)\n",
    "        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n",
    "        dominant_blues.append(dominant_blue)\n",
    "        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n",
    "        dominant_greens.append(dominant_green)\n",
    "        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n",
    "        dominant_reds.append(dominant_red)\n",
    "        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "        dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "        dominant_scores.append(dominant_score)\n",
    "        if data.get('labelAnnotations'):\n",
    "            label_description = data['labelAnnotations'][0]['description']\n",
    "            label_descriptions.append(label_description)\n",
    "            label_score = data['labelAnnotations'][0]['score']\n",
    "            label_scores.append(label_score)\n",
    "        else:\n",
    "            nl_count += 1\n",
    "            label_descriptions.append('nothing')\n",
    "            label_scores.append(-1)\n",
    "    except FileNotFoundError:\n",
    "        nf_count += 1\n",
    "        vertex_xs.append(-1)\n",
    "        vertex_ys.append(-1)\n",
    "        bounding_confidences.append(-1)\n",
    "        bounding_importance_fracs.append(-1)\n",
    "        dominant_blues.append(-1)\n",
    "        dominant_greens.append(-1)\n",
    "        dominant_reds.append(-1)\n",
    "        dominant_pixel_fracs.append(-1)\n",
    "        dominant_scores.append(-1)\n",
    "        label_descriptions.append('nothing')\n",
    "        label_scores.append(-1)\n",
    "\n",
    "print('test - not found: ', nf_count)\n",
    "test.loc[:, 'vertex_x'] = vertex_xs\n",
    "test.loc[:, 'vertex_y'] = vertex_ys\n",
    "test.loc[:, 'bounding_confidence'] = bounding_confidences\n",
    "test.loc[:, 'bounding_importance'] = bounding_importance_fracs\n",
    "test.loc[:, 'dominant_blue'] = dominant_blues\n",
    "test.loc[:, 'dominant_green'] = dominant_greens\n",
    "test.loc[:, 'dominant_red'] = dominant_reds\n",
    "test.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "test.loc[:, 'dominant_score'] = dominant_scores\n",
    "test.loc[:, 'label_description'] = label_descriptions\n",
    "test.loc[:, 'label_score'] = label_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['Name', 'RescuerID', 'Description'], axis=1, inplace=True)\n",
    "test.drop(['Name', 'RescuerID', 'Description'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14720, 152)\n",
      "(3948, 152)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Age</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Breed2</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>Color3</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>...</th>\n",
       "      <th>vertex_y</th>\n",
       "      <th>bounding_confidence</th>\n",
       "      <th>bounding_importance</th>\n",
       "      <th>dominant_blue</th>\n",
       "      <th>dominant_green</th>\n",
       "      <th>dominant_red</th>\n",
       "      <th>dominant_pixel_frac</th>\n",
       "      <th>dominant_score</th>\n",
       "      <th>label_description</th>\n",
       "      <th>label_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>nothing</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>nothing</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>nothing</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>241</td>\n",
       "      <td>241</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>nothing</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>nothing</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 152 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Type  Age Breed1 Breed2 Gender Color1 Color2 Color3 MaturitySize FurLength  \\\n",
       "0    2    6    265      0      1      4      7      0            2         2   \n",
       "1    1    2    307      0      2      1      2      7            2         2   \n",
       "2    1    2    307      0      2      1      0      0            2         1   \n",
       "3    2    2    241    241      1      1      4      6            2         1   \n",
       "4    1    1    307      0      1      2      5      0            2         1   \n",
       "\n",
       "      ...      vertex_y bounding_confidence bounding_importance dominant_blue  \\\n",
       "0     ...            -1                  -1                  -1            -1   \n",
       "1     ...            -1                  -1                  -1            -1   \n",
       "2     ...            -1                  -1                  -1            -1   \n",
       "3     ...            -1                  -1                  -1            -1   \n",
       "4     ...            -1                  -1                  -1            -1   \n",
       "\n",
       "   dominant_green  dominant_red dominant_pixel_frac  dominant_score  \\\n",
       "0              -1            -1                  -1              -1   \n",
       "1              -1            -1                  -1              -1   \n",
       "2              -1            -1                  -1              -1   \n",
       "3              -1            -1                  -1              -1   \n",
       "4              -1            -1                  -1              -1   \n",
       "\n",
       "   label_description  label_score  \n",
       "0            nothing           -1  \n",
       "1            nothing           -1  \n",
       "2            nothing           -1  \n",
       "3            nothing           -1  \n",
       "4            nothing           -1  \n",
       "\n",
       "[5 rows x 152 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_cols = ['Age', 'Quantity', 'Fee', 'VideoAmt', \n",
    "                'PhotoAmt', 'AdoptionSpeed', 'doc_sent_mag', \n",
    "                'doc_sent_score', 'dominant_score', 'dominant_pixel_frac', \n",
    "                'dominant_red', 'dominant_green', 'dominant_blue', \n",
    "                'bounding_importance', 'bounding_confidence', \n",
    "                'vertex_x', 'vertex_y', 'label_score'] + ['svd_{}'.format(i) for i in range(120)]\n",
    "cat_cols = list(set(train.columns) - set(numeric_cols))\n",
    "train.loc[:, cat_cols] = train[cat_cols].astype('category')\n",
    "test.loc[:, cat_cols] = test[cat_cols].astype('category')\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the categorical features\n",
    "foo = train.dtypes\n",
    "cat_feature_names = foo[foo == \"category\"]\n",
    "cat_features = [train.columns.get_loc(c) for c in train.columns if c in cat_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started lgb fold 1/20\n",
      "Prep LGB\n",
      "Train LGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 16, 150]\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "D:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 1.05925\tvalid_1's rmse: 1.09082\n",
      "[200]\ttraining's rmse: 1.00427\tvalid_1's rmse: 1.06733\n",
      "[300]\ttraining's rmse: 0.970098\tvalid_1's rmse: 1.05597\n",
      "[400]\ttraining's rmse: 0.94153\tvalid_1's rmse: 1.051\n",
      "[500]\ttraining's rmse: 0.919741\tvalid_1's rmse: 1.04827\n",
      "[600]\ttraining's rmse: 0.89822\tvalid_1's rmse: 1.0456\n",
      "[700]\ttraining's rmse: 0.878767\tvalid_1's rmse: 1.04311\n",
      "[800]\ttraining's rmse: 0.859662\tvalid_1's rmse: 1.04189\n",
      "[900]\ttraining's rmse: 0.840928\tvalid_1's rmse: 1.04051\n",
      "[1000]\ttraining's rmse: 0.825056\tvalid_1's rmse: 1.03939\n",
      "[1100]\ttraining's rmse: 0.809546\tvalid_1's rmse: 1.03906\n",
      "Early stopping, best iteration is:\n",
      "[1071]\ttraining's rmse: 0.814171\tvalid_1's rmse: 1.03883\n",
      "Predict 1/2\n",
      "Valid Counts =  Counter({4: 207, 2: 198, 3: 161, 1: 152, 0: 20})\n",
      "Predicted Counts =  Counter({2.0: 269, 3.0: 205, 4.0: 189, 1.0: 75})\n",
      "Coefficients =  [0.44934045 1.88459166 2.46957949 2.90371334]\n",
      "QWK =  0.46060830151550536\n",
      "Predict 2/2\n",
      "lgb cv score 1: RMSE 1.0388327022441557 QWK 0.46060830151550536\n",
      "Started lgb fold 2/20\n",
      "Prep LGB\n",
      "Train LGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 16, 150]\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "D:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 1.0574\tvalid_1's rmse: 1.09666\n",
      "[200]\ttraining's rmse: 1.00332\tvalid_1's rmse: 1.07379\n",
      "[300]\ttraining's rmse: 0.968671\tvalid_1's rmse: 1.06075\n",
      "[400]\ttraining's rmse: 0.941306\tvalid_1's rmse: 1.05258\n",
      "[500]\ttraining's rmse: 0.919759\tvalid_1's rmse: 1.04688\n",
      "[600]\ttraining's rmse: 0.900173\tvalid_1's rmse: 1.04191\n",
      "[700]\ttraining's rmse: 0.880712\tvalid_1's rmse: 1.0387\n",
      "[800]\ttraining's rmse: 0.862765\tvalid_1's rmse: 1.03593\n",
      "[900]\ttraining's rmse: 0.84515\tvalid_1's rmse: 1.03308\n",
      "[1000]\ttraining's rmse: 0.828278\tvalid_1's rmse: 1.03157\n",
      "[1100]\ttraining's rmse: 0.812428\tvalid_1's rmse: 1.02952\n",
      "[1200]\ttraining's rmse: 0.796204\tvalid_1's rmse: 1.02755\n",
      "[1300]\ttraining's rmse: 0.781339\tvalid_1's rmse: 1.02632\n",
      "[1400]\ttraining's rmse: 0.767407\tvalid_1's rmse: 1.02531\n",
      "[1500]\ttraining's rmse: 0.753012\tvalid_1's rmse: 1.02426\n",
      "[1600]\ttraining's rmse: 0.739333\tvalid_1's rmse: 1.02372\n",
      "[1700]\ttraining's rmse: 0.725536\tvalid_1's rmse: 1.02318\n",
      "[1800]\ttraining's rmse: 0.711252\tvalid_1's rmse: 1.02295\n",
      "Early stopping, best iteration is:\n",
      "[1762]\ttraining's rmse: 0.716729\tvalid_1's rmse: 1.02277\n",
      "Predict 1/2\n",
      "Valid Counts =  Counter({4: 207, 2: 198, 3: 161, 1: 152, 0: 20})\n",
      "Predicted Counts =  Counter({3.0: 300, 2.0: 170, 1.0: 138, 4.0: 129, 0.0: 1})\n",
      "Coefficients =  [0.48871991 2.05469781 2.37589094 3.04409574]\n",
      "QWK =  0.4768330804696588\n",
      "Predict 2/2\n",
      "lgb cv score 2: RMSE 1.0227745898154166 QWK 0.4768330804696588\n",
      "Started lgb fold 3/20\n",
      "Prep LGB\n",
      "Train LGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 16, 150]\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "D:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 1.05793\tvalid_1's rmse: 1.10004\n",
      "[200]\ttraining's rmse: 1.00284\tvalid_1's rmse: 1.07712\n",
      "[300]\ttraining's rmse: 0.968224\tvalid_1's rmse: 1.06651\n",
      "[400]\ttraining's rmse: 0.941628\tvalid_1's rmse: 1.05991\n",
      "[500]\ttraining's rmse: 0.920193\tvalid_1's rmse: 1.05492\n",
      "[600]\ttraining's rmse: 0.899802\tvalid_1's rmse: 1.05243\n",
      "[700]\ttraining's rmse: 0.879944\tvalid_1's rmse: 1.0497\n",
      "[800]\ttraining's rmse: 0.861032\tvalid_1's rmse: 1.04826\n",
      "[900]\ttraining's rmse: 0.843451\tvalid_1's rmse: 1.04704\n",
      "[1000]\ttraining's rmse: 0.827867\tvalid_1's rmse: 1.04707\n",
      "[1100]\ttraining's rmse: 0.812885\tvalid_1's rmse: 1.04684\n",
      "Early stopping, best iteration is:\n",
      "[1065]\ttraining's rmse: 0.81773\tvalid_1's rmse: 1.04671\n",
      "Predict 1/2\n",
      "Valid Counts =  Counter({4: 207, 2: 198, 3: 160, 1: 152, 0: 20})\n",
      "Predicted Counts =  Counter({2.0: 504, 4.0: 219, 1.0: 14})\n",
      "Coefficients =  [0.50954649 1.65560666 2.74753949 2.6029551 ]\n",
      "QWK =  0.3989703801535819\n",
      "Predict 2/2\n",
      "lgb cv score 3: RMSE 1.046709656142876 QWK 0.3989703801535819\n",
      "Started lgb fold 4/20\n",
      "Prep LGB\n",
      "Train LGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 16, 150]\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "D:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 1.05838\tvalid_1's rmse: 1.09894\n"
     ]
    }
   ],
   "source": [
    "def run_cv_model(train, test, target, model_fn, params={}, eval_fn=None, label='model'):\n",
    "    kf = StratifiedKFold(n_splits=20, random_state=42, shuffle=True)\n",
    "    fold_splits = kf.split(train, target)\n",
    "    cv_scores = []\n",
    "    qwk_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros((train.shape[0], 5))\n",
    "    all_coefficients = np.zeros((5, 4))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    i = 1\n",
    "    for dev_index, val_index in fold_splits:\n",
    "        print('Started ' + label + ' fold ' + str(i) + '/20')\n",
    "        if isinstance(train, pd.DataFrame):\n",
    "            dev_X, val_X = train.iloc[dev_index], train.iloc[val_index]\n",
    "            dev_y, val_y = target[dev_index], target[val_index]\n",
    "        else:\n",
    "            dev_X, val_X = train[dev_index], train[val_index]\n",
    "            dev_y, val_y = target[dev_index], target[val_index]\n",
    "        params2 = params.copy()\n",
    "        pred_val_y, pred_test_y, importances, coefficients, qwk = model_fn(dev_X, dev_y, val_X, val_y, test, params2)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index] = pred_val_y\n",
    "        all_coefficients[i-1, :] = coefficients\n",
    "        if eval_fn is not None:\n",
    "            cv_score = eval_fn(val_y, pred_val_y)\n",
    "            cv_scores.append(cv_score)\n",
    "            qwk_scores.append(qwk)\n",
    "            print(label + ' cv score {}: RMSE {} QWK {}'.format(i, cv_score, qwk))\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature'] = train.columns.values\n",
    "        fold_importance_df['importance'] = importances\n",
    "        fold_importance_df['fold'] = i\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)        \n",
    "        i += 1\n",
    "    print('{} cv RMSE scores : {}'.format(label, cv_scores))\n",
    "    print('{} cv mean RMSE score : {}'.format(label, np.mean(cv_scores)))\n",
    "    print('{} cv std RMSE score : {}'.format(label, np.mean(cv_scores)))\n",
    "    print('{} cv QWK scores : {}'.format(label, qwk_scores))\n",
    "    print('{} cv mean QWK score : {}'.format(label, np.mean(qwk_scores)))\n",
    "    print('{} cv std QWK score : {}'.format(label, np.std(qwk_scores)))\n",
    "    pred_full_test = pred_full_test / 5.0\n",
    "    results = {'label': label,\n",
    "               'train': pred_train, 'test': pred_full_test,\n",
    "                'cv': cv_scores, 'qwk': qwk_scores,\n",
    "               'importance': feature_importance_df,\n",
    "               'coefficients': all_coefficients}\n",
    "    return results\n",
    "\n",
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 70,\n",
    "          'max_depth': 8,\n",
    "          'learning_rate': 0.01,\n",
    "          'bagging_fraction': 0.85,\n",
    "          'feature_fraction': 0.8,\n",
    "          'min_split_gain': 0.02,\n",
    "          'min_child_samples': 150,\n",
    "          'min_child_weight': 0.02,\n",
    "          'lambda_l2': 0.0475,\n",
    "          'verbosity': -1,\n",
    "          'data_random_seed': 17,\n",
    "          'early_stop': 100,\n",
    "          'verbose_eval': 100,\n",
    "          'num_rounds': 10000}\n",
    "\n",
    "def runLGB(train_X, train_y, test_X, test_y, test_X2, params):\n",
    "    print('Prep LGB')\n",
    "    d_train = lgb.Dataset(train_X, label=train_y)\n",
    "    d_valid = lgb.Dataset(test_X, label=test_y)\n",
    "    watchlist = [d_train, d_valid]\n",
    "    print('Train LGB')\n",
    "    num_rounds = params.pop('num_rounds')\n",
    "    verbose_eval = params.pop('verbose_eval')\n",
    "    early_stop = None\n",
    "    if params.get('early_stop'):\n",
    "        early_stop = params.pop('early_stop')\n",
    "    model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=num_rounds,\n",
    "                      valid_sets=watchlist,\n",
    "                      verbose_eval=verbose_eval,\n",
    "                      categorical_feature=list(cat_features),\n",
    "                      early_stopping_rounds=early_stop)\n",
    "    \n",
    "    print('Predict 1/2')\n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    optR = OptimizedRounder()\n",
    "    optR.fit(pred_test_y, test_y)\n",
    "    coefficients = optR.coefficients()\n",
    "    pred_test_y_k = optR.predict(pred_test_y, coefficients)\n",
    "    print(\"Valid Counts = \", Counter(test_y))\n",
    "    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "    print(\"Coefficients = \", coefficients)\n",
    "    qwk = quadratic_weighted_kappa(test_y, pred_test_y_k)\n",
    "    print(\"QWK = \", qwk)\n",
    "    print('Predict 2/2')\n",
    "    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n",
    "    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), model.feature_importance(), coefficients, qwk\n",
    "\n",
    "results = run_cv_model(train, test, target, runLGB, params, rmse, 'lgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imports = results['importance'].groupby('feature')['feature', 'importance'].mean().reset_index()\n",
    "imports.sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optR = OptimizedRounder()\n",
    "coefficients_ = np.mean(results['coefficients'], axis=0)\n",
    "print(coefficients_)\n",
    "# manually adjust coefs\n",
    "coefficients_[0] = 1.64\n",
    "coefficients_[1] = 2.11\n",
    "coefficients_[3] = 2.85\n",
    "train_predictions = [r[0] for r in results['train']]\n",
    "train_predictions = optR.predict(train_predictions, coefficients_).astype(int)\n",
    "Counter(train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optR = OptimizedRounder()\n",
    "coefficients_ = np.mean(results['coefficients'], axis=0)\n",
    "print(coefficients_)\n",
    "# manually adjust coefs\n",
    "coefficients_[0] = 1.645\n",
    "coefficients_[1] = 2.115\n",
    "coefficients_[3] = 2.84\n",
    "test_predictions = [r[0] for r in results['test']]\n",
    "test_predictions = optR.predict(test_predictions, coefficients_).astype(int)\n",
    "Counter(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True Distribution:\")\n",
    "print(pd.value_counts(target, normalize=True).sort_index())\n",
    "print(\"Test Predicted Distribution:\")\n",
    "print(pd.value_counts(test_predictions, normalize=True).sort_index())\n",
    "print(\"Train Predicted Distribution:\")\n",
    "print(pd.value_counts(train_predictions, normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sk_cmatrix(target, train_predictions), index=list(range(5)), columns=list(range(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadratic_weighted_kappa(target, train_predictions)\n",
    "rmse(target, [r[0] for r in results['train']])\n",
    "submission = pd.DataFrame({'PetID': test_id, 'AdoptionSpeed': test_predictions})\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
