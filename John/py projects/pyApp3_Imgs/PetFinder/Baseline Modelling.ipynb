{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_rows = 128\n",
    "pd.options.display.max_columns = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data \n",
    "os.listdir('input/test/')\n",
    "train = pd.read_csv('input/train/train.csv')\n",
    "test = pd.read_csv('input/test/test.csv')\n",
    "sample_submission = pd.read_csv('input/test/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load mapping dictionaries\n",
    "labels_breed = pd.read_csv('input/breed_labels.csv')\n",
    "labels_state = pd.read_csv('input/color_labels.csv')\n",
    "labels_color = pd.read_csv('input/state_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "additional data:\n",
    "We have also additional information about pets available in form of:\n",
    "\n",
    "images\n",
    "metadata\n",
    "sentiment\n",
    "Integration of those will enable us to possibly improve the score. Information derived from example from images should be very important, as picture of a pet influences the way we look at an animal in a significant way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of train images files: 2600\n",
      "num of train metadata files: 58311\n",
      "num of train sentiment files: 14442\n",
      "num of test images files: 15040\n",
      "num of test metadata files: 15040\n",
      "num of test sentiment files: 3815\n"
     ]
    }
   ],
   "source": [
    "train_image_files = sorted(glob.glob('input/train/train_images/*.jpg'))\n",
    "train_metadata_files = sorted(glob.glob('input/train/train_metadata/*.json'))\n",
    "train_sentiment_files = sorted(glob.glob('input/train/train_sentiment/*.json'))\n",
    "\n",
    "print('num of train images files: {}'.format(len(train_image_files)))\n",
    "print('num of train metadata files: {}'.format(len(train_metadata_files)))\n",
    "print('num of train sentiment files: {}'.format(len(train_sentiment_files)))\n",
    "\n",
    "\n",
    "test_image_files = sorted(glob.glob('input/test/test_images/*.jpg'))\n",
    "test_metadata_files = sorted(glob.glob('input/test/test_metadata/*.json'))\n",
    "test_sentiment_files = sorted(glob.glob('input/test/test_sentiment/*.json'))\n",
    "\n",
    "print('num of test images files: {}'.format(len(test_image_files)))\n",
    "print('num of test metadata files: {}'.format(len(test_metadata_files)))\n",
    "print('num of test sentiment files: {}'.format(len(test_sentiment_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14720, 1)\n",
      "670\n",
      "fraction of pets with images: 0.046\n",
      "14652\n",
      "fraction of pets with metadata: 0.995\n",
      "14442\n",
      "fraction of pets with sentiment: 0.981\n"
     ]
    }
   ],
   "source": [
    "#train analysis\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "# Images:\n",
    "train_df_ids = train[['PetID']]\n",
    "print(train_df_ids.shape)\n",
    "\n",
    "train_df_imgs = pd.DataFrame(train_image_files)\n",
    "train_df_imgs.columns = ['image_filename']\n",
    "train_imgs_pets = train_df_imgs['image_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\n",
    "train_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\n",
    "\n",
    "unique_pets_imgs = len(train_imgs_pets.unique())\n",
    "print(unique_pets_imgs)\n",
    "#unique_pets_csv = len(train_df_ids['PetID'].unique())\n",
    "#print(unique_pets_csv)\n",
    "#use in1d instead of intersect1d\n",
    "\n",
    "pets_with_images = len(np.in1d(train_imgs_pets.unique(), train_df_ids['PetID'].unique()))\n",
    "print('fraction of pets with images: {:.3f}'.format(pets_with_images / train_df_ids.shape[0]))\n",
    "\n",
    "# Metadata:\n",
    "train_df_ids = train[['PetID']]\n",
    "train_df_metadata = pd.DataFrame(train_metadata_files)\n",
    "train_df_metadata.columns = ['metadata_filename']\n",
    "train_metadata_pets = train_df_metadata['metadata_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\n",
    "train_df_metadata = train_df_metadata.assign(PetID=train_metadata_pets)\n",
    "print(len(train_metadata_pets.unique()))\n",
    "\n",
    "pets_with_metadatas = len(np.in1d(train_metadata_pets.unique(), train_df_ids['PetID'].unique()))\n",
    "print('fraction of pets with metadata: {:.3f}'.format(pets_with_metadatas / train_df_ids.shape[0]))\n",
    "\n",
    "# Sentiment:\n",
    "train_df_ids = train[['PetID']]\n",
    "train_df_sentiment = pd.DataFrame(train_sentiment_files)\n",
    "train_df_sentiment.columns = ['sentiment_filename']\n",
    "train_sentiment_pets = train_df_sentiment['sentiment_filename'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "train_df_sentiment = train_df_sentiment.assign(PetID=train_sentiment_pets)\n",
    "print(len(train_sentiment_pets.unique()))\n",
    "\n",
    "pets_with_sentiments = len(np.in1d(train_sentiment_pets.unique(), train_df_ids['PetID'].unique()))\n",
    "print('fraction of pets with sentiment: {:.3f}'.format(pets_with_sentiments / train_df_ids.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3948, 1)\n",
      "3821\n",
      "fraction of pets with images: 0.968\n",
      "3821\n",
      "fraction of pets with metadata: 0.968\n",
      "3815\n",
      "fraction of pets with sentiment: 0.966\n",
      "images and metadata distributions the same? False\n"
     ]
    }
   ],
   "source": [
    "# Images:\n",
    "test_df_ids = test[['PetID']]\n",
    "print(test_df_ids.shape)\n",
    "\n",
    "test_df_imgs = pd.DataFrame(test_image_files)\n",
    "test_df_imgs.columns = ['image_filename']\n",
    "test_imgs_pets = test_df_imgs['image_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\n",
    "test_df_imgs = test_df_imgs.assign(PetID=test_imgs_pets)\n",
    "print(len(test_imgs_pets.unique()))\n",
    "\n",
    "pets_with_images = len(np.in1d(test_imgs_pets.unique(), test_df_ids['PetID'].unique()))\n",
    "print('fraction of pets with images: {:.3f}'.format(pets_with_images / test_df_ids.shape[0]))\n",
    "\n",
    "\n",
    "# Metadata:\n",
    "test_df_ids = test[['PetID']]\n",
    "test_df_metadata = pd.DataFrame(test_metadata_files)\n",
    "test_df_metadata.columns = ['metadata_filename']\n",
    "test_metadata_pets = test_df_metadata['metadata_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\n",
    "test_df_metadata = test_df_metadata.assign(PetID=test_metadata_pets)\n",
    "print(len(test_metadata_pets.unique()))\n",
    "\n",
    "pets_with_metadatas = len(np.in1d(test_metadata_pets.unique(), test_df_ids['PetID'].unique()))\n",
    "print('fraction of pets with metadata: {:.3f}'.format(pets_with_metadatas / test_df_ids.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "# Sentiment:\n",
    "test_df_ids = test[['PetID']]\n",
    "test_df_sentiment = pd.DataFrame(test_sentiment_files)\n",
    "test_df_sentiment.columns = ['sentiment_filename']\n",
    "test_sentiment_pets = test_df_sentiment['sentiment_filename'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "test_df_sentiment = test_df_sentiment.assign(PetID=test_sentiment_pets)\n",
    "print(len(test_sentiment_pets.unique()))\n",
    "\n",
    "pets_with_sentiments = len(np.in1d(test_sentiment_pets.unique(), test_df_ids['PetID'].unique()))\n",
    "print('fraction of pets with sentiment: {:.3f}'.format(pets_with_sentiments / test_df_ids.shape[0]))\n",
    "\n",
    "\n",
    "# are distributions the same?\n",
    "print('images and metadata distributions the same? {}'.format(\n",
    "    np.all(test_metadata_pets == test_imgs_pets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetFinderParser(object):\n",
    "    \n",
    "    def __init__(self, debug=False):\n",
    "        \n",
    "        self.debug = debug\n",
    "        self.sentence_sep = ' '\n",
    "        \n",
    "        # Does not have to be extracted because main DF already contains description\n",
    "        self.extract_sentiment_text = False\n",
    "        \n",
    "        \n",
    "    def open_metadata_file(self, filename):\n",
    "        \"\"\"\n",
    "        Load metadata file.\n",
    "        \"\"\"\n",
    "        with open(filename, 'r') as f:\n",
    "            metadata_file = json.load(f)\n",
    "        return metadata_file\n",
    "            \n",
    "    def open_sentiment_file(self, filename):\n",
    "        \"\"\"\n",
    "        Load sentiment file.\n",
    "        \"\"\"\n",
    "        with open(filename, 'r') as f:\n",
    "            sentiment_file = json.load(f)\n",
    "        return sentiment_file\n",
    "            \n",
    "    def open_image_file(self, filename):\n",
    "        \"\"\"\n",
    "        Load image file.\n",
    "        \"\"\"\n",
    "        image = np.asarray(Image.open(filename))\n",
    "        return image\n",
    "        \n",
    "    def parse_sentiment_file(self, file):\n",
    "        \"\"\"\n",
    "        Parse sentiment file. Output DF with sentiment features.\n",
    "        \"\"\"\n",
    "        \n",
    "        file_sentiment = file['documentSentiment']\n",
    "        file_entities = [x['name'] for x in file['entities']]\n",
    "        file_entities = self.sentence_sep.join(file_entities)\n",
    "\n",
    "        if self.extract_sentiment_text:\n",
    "            file_sentences_text = [x['text']['content'] for x in file['sentences']]\n",
    "            file_sentences_text = self.sentence_sep.join(file_sentences_text)\n",
    "        file_sentences_sentiment = [x['sentiment'] for x in file['sentences']]\n",
    "        \n",
    "        file_sentences_sentiment = pd.DataFrame.from_dict(\n",
    "            file_sentences_sentiment, orient='columns').sum()\n",
    "        file_sentences_sentiment = file_sentences_sentiment.add_prefix('document_').to_dict()\n",
    "        \n",
    "        file_sentiment.update(file_sentences_sentiment)\n",
    "        \n",
    "        df_sentiment = pd.DataFrame.from_dict(file_sentiment, orient='index').T\n",
    "        if self.extract_sentiment_text:\n",
    "            df_sentiment['text'] = file_sentences_text\n",
    "            \n",
    "        df_sentiment['entities'] = file_entities\n",
    "        df_sentiment = df_sentiment.add_prefix('sentiment_')\n",
    "        \n",
    "        return df_sentiment\n",
    "    \n",
    "    def parse_metadata_file(self, file):\n",
    "        \"\"\"\n",
    "        Parse metadata file. Output DF with metadata features.\n",
    "        \"\"\"\n",
    "        \n",
    "        file_keys = list(file.keys())\n",
    "        \n",
    "        if 'labelAnnotations' in file_keys:\n",
    "            file_annots = file['labelAnnotations'][:int(len(file['labelAnnotations']) * 0.3)]\n",
    "            file_top_score = np.asarray([x['score'] for x in file_annots]).mean()\n",
    "            file_top_desc = [x['description'] for x in file_annots]\n",
    "        else:\n",
    "            file_top_score = np.nan\n",
    "            file_top_desc = ['']\n",
    "        \n",
    "        file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']\n",
    "        file_crops = file['cropHintsAnnotation']['cropHints']\n",
    "\n",
    "        file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n",
    "        file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n",
    "\n",
    "        file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()\n",
    "        \n",
    "        if 'importanceFraction' in file_crops[0].keys():\n",
    "            file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n",
    "        else:\n",
    "            file_crop_importance = np.nan\n",
    "\n",
    "        df_metadata = {\n",
    "            'annots_score': file_top_score,\n",
    "            'color_score': file_color_score,\n",
    "            'color_pixelfrac': file_color_pixelfrac,\n",
    "            'crop_conf': file_crop_conf,\n",
    "            'crop_importance': file_crop_importance,\n",
    "            'annots_top_desc': self.sentence_sep.join(file_top_desc)\n",
    "        }\n",
    "        \n",
    "        df_metadata = pd.DataFrame.from_dict(df_metadata, orient='index').T\n",
    "        df_metadata = df_metadata.add_prefix('metadata_')\n",
    "        \n",
    "        return df_metadata\n",
    "    \n",
    "\n",
    "# Helper function for parallel data processing:\n",
    "def extract_additional_features(pet_id, mode='train'):\n",
    "    \n",
    "    sentiment_filename = '../input/{}_sentiment/{}.json'.format(mode, pet_id)\n",
    "    try:\n",
    "        sentiment_file = pet_parser.open_sentiment_file(sentiment_filename)\n",
    "        df_sentiment = pet_parser.parse_sentiment_file(sentiment_file)\n",
    "        df_sentiment['PetID'] = pet_id\n",
    "    except FileNotFoundError:\n",
    "        df_sentiment = []\n",
    "\n",
    "    dfs_metadata = []\n",
    "    metadata_filenames = sorted(glob.glob('../input/{}_metadata/{}*.json'.format(mode, pet_id)))\n",
    "    if len(metadata_filenames) > 0:\n",
    "        for f in metadata_filenames:\n",
    "            metadata_file = pet_parser.open_metadata_file(f)\n",
    "            df_metadata = pet_parser.parse_metadata_file(metadata_file)\n",
    "            df_metadata['PetID'] = pet_id\n",
    "            dfs_metadata.append(df_metadata)\n",
    "        dfs_metadata = pd.concat(dfs_metadata, ignore_index=True, sort=False)\n",
    "    dfs = [df_sentiment, dfs_metadata]\n",
    "    \n",
    "    return dfs\n",
    "\n",
    "\n",
    "pet_parser = PetFinderParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  42 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=6)]: Done 14720 out of 14720 | elapsed:    2.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,) (0,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done 804 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=6)]: Done 3948 out of 3948 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,) (0,)\n"
     ]
    }
   ],
   "source": [
    "# Unique IDs from train and test:\n",
    "debug = False\n",
    "train_pet_ids = train.PetID.unique()\n",
    "test_pet_ids = test.PetID.unique()\n",
    "\n",
    "if debug:\n",
    "    train_pet_ids = train_pet_ids[:1000]\n",
    "    test_pet_ids = test_pet_ids[:500]\n",
    "\n",
    "\n",
    "# Train set:\n",
    "# Parallel processing of data:\n",
    "dfs_train = Parallel(n_jobs=6, verbose=1)(\n",
    "    delayed(extract_additional_features)(i, mode='train') for i in train_pet_ids)\n",
    "\n",
    "# Extract processed data and format them as DFs:\n",
    "train_dfs_sentiment = [x[0] for x in dfs_train if isinstance(x[0], pd.DataFrame)]\n",
    "train_dfs_metadata = [x[1] for x in dfs_train if isinstance(x[1], pd.DataFrame)]\n",
    "\n",
    "train_dfs_sentiment = pd.concat(train_dfs_sentiment, ignore_index=True, sort=False)\n",
    "train_dfs_metadata = pd.concat(train_dfs_metadata, ignore_index=True, sort=False)\n",
    "\n",
    "#train_dfs_sentiment = np.array(train_dfs_sentiment)\n",
    "#train_dfs_metadata = np.array(train_dfs_metadata)\n",
    "\n",
    "print(train_dfs_sentiment.shape, train_dfs_metadata.shape)\n",
    "\n",
    "\n",
    "# Test set:\n",
    "# Parallel processing of data:\n",
    "dfs_test = Parallel(n_jobs=6, verbose=1)(\n",
    "    delayed(extract_additional_features)(i, mode='test') for i in test_pet_ids)\n",
    "\n",
    "# Extract processed data and format them as DFs:\n",
    "test_dfs_sentiment = [x[0] for x in dfs_test if isinstance(x[0], pd.DataFrame)]\n",
    "test_dfs_metadata = [x[1] for x in dfs_test if isinstance(x[1], pd.DataFrame)]\n",
    "\n",
    "test_dfs_sentiment = pd.concat(test_dfs_sentiment, ignore_index=True, sort=False)\n",
    "test_dfs_metadata = pd.concat(test_dfs_metadata, ignore_index=True, sort=False)\n",
    "\n",
    "test_dfs_sentiment = np.array(test_dfs_sentiment)\n",
    "test_dfs_metadata = np.array(test_dfs_metadata)\n",
    "\n",
    "print(test_dfs_sentiment.shape, test_dfs_metadata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
