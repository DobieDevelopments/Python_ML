{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import lightgbm as lgb\n",
    "#A fast, distributed, high performance gradient boosting (GBDT, GBRT, GBM or MART) framework \n",
    "#based on decision tree algorithms, used for ranking, classification and many other machine learning tasks.\n",
    "np.random.seed(369)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following 3 functions have been taken from Ben Hamner's github repository\n",
    "# https://github.com/benhamner/Metrics\n",
    "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = y\n",
    "    rater_b = y_pred\n",
    "    min_rating=None\n",
    "    max_rating=None\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return (1.0 - numerator / denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        ll = quadratic_weighted_kappa(y, X_p)\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(actual, predicted):\n",
    "    return sqrt(mean_squared_error(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "(14720, 24)\n",
      "Test\n",
      "(3948, 23)\n",
      "Breeds\n",
      "(307, 3)\n",
      "Colors\n",
      "(7, 2)\n",
      "States\n",
      "(15, 2)\n"
     ]
    }
   ],
   "source": [
    "#Load all CSV data, printing the rows and columns.\n",
    "print('Train')\n",
    "train = pd.read_csv(\"input/train/train.csv\")\n",
    "print(train.shape)\n",
    "\n",
    "print('Test')\n",
    "test = pd.read_csv(\"input/test/test.csv\")\n",
    "print(test.shape)\n",
    "\n",
    "print('Breeds')\n",
    "breeds = pd.read_csv(\"input/breed_labels.csv\")\n",
    "print(breeds.shape)\n",
    "\n",
    "print('Colors')\n",
    "colors = pd.read_csv(\"input/color_labels.csv\")\n",
    "print(colors.shape)\n",
    "\n",
    "print('States')\n",
    "states = pd.read_csv(\"input/state_labels.csv\")\n",
    "print(states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up arrays for the CSV columns that we want\n",
    "target = train['AdoptionSpeed']\n",
    "train_id = train['PetID']\n",
    "test_id = test['PetID']\n",
    "train.drop(['AdoptionSpeed', 'PetID'], axis=1, inplace=True)\n",
    "test.drop(['PetID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup sentiment magnitudes and scores arrays\n",
    "doc_sent_mag = []\n",
    "doc_sent_score = []\n",
    "nf_count = 0\n",
    "for pet in train_id:\n",
    "    try:\n",
    "        with open('input/train/train_sentiment/' + '.json', 'r') as f:\n",
    "            sentiment = json.load(f)\n",
    "        doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "        doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "    except FileNotFoundError:\n",
    "        nf_count += 1\n",
    "        doc_sent_mag.append(-1)\n",
    "        doc_sent_score.append(-1)\n",
    "\n",
    "train.loc[:, 'doc_sent_mag'] = doc_sent_mag\n",
    "train.loc[:, 'doc_sent_score'] = doc_sent_score\n",
    "\n",
    "doc_sent_mag = []\n",
    "doc_sent_score = []\n",
    "nf_count = 0\n",
    "for pet in test_id:\n",
    "    try:\n",
    "        with open('input/test/test_sentiment/' + '.json', 'r') as f:\n",
    "            sentiment = json.load(f)\n",
    "        doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "        doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "    except FileNotFoundError:\n",
    "        nf_count += 1\n",
    "        doc_sent_mag.append(-1)\n",
    "        doc_sent_score.append(-1)\n",
    "\n",
    "test.loc[:, 'doc_sent_mag'] = doc_sent_mag\n",
    "test.loc[:, 'doc_sent_score'] = doc_sent_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (tfidf): (14720, 178520)\n",
      "X (svd): (14720, 200)\n",
      "train: (14720, 24)\n"
     ]
    }
   ],
   "source": [
    "# sci-kit learn TFIDF Vectorizer is used for text feature extraction\n",
    "# https://stackoverflow.com/questions/36800654/how-is-the-tfidfvectorizer-in-scikit-learn-supposed-to-work\n",
    "# Truncated SVD is used to reduce dimensionality\n",
    "train_desc = train.Description.fillna(\"none\").values\n",
    "test_desc = test.Description.fillna(\"none\").values\n",
    "\n",
    "tfv = TfidfVectorizer(min_df=2,  max_features=None,\n",
    "        strip_accents='unicode', analyzer='word', token_pattern=r'(?u)\\b\\w+\\b',\n",
    "        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "        )\n",
    "    \n",
    "# Fit TFIDF\n",
    "tfv.fit(list(train_desc))\n",
    "X =  tfv.transform(train_desc)\n",
    "X_test = tfv.transform(test_desc)\n",
    "print(\"X (tfidf):\", X.shape)\n",
    "\n",
    "svd = TruncatedSVD(n_components=200)\n",
    "svd.fit(X)\n",
    "# print(svd.explained_variance_ratio_.sum())\n",
    "# print(svd.explained_variance_ratio_)\n",
    "X = svd.transform(X)\n",
    "print(\"X (svd):\", X.shape)\n",
    "\n",
    "# X = pd.DataFrame(X, columns=['svd_{}'.format(i) for i in range(120)])\n",
    "# train = pd.concat((train, X), axis=1)\n",
    "# X_test = svd.transform(X_test)\n",
    "# X_test = pd.DataFrame(X_test, columns=['svd_{}'.format(i) for i in range(120)])\n",
    "# test = pd.concat((test, X_test), axis=1)\n",
    "\n",
    "print(\"train:\", train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (tfidf): (14720, 10000)\n",
      "X (svd): (14720, 120)\n",
      "train: (14720, 144)\n"
     ]
    }
   ],
   "source": [
    "## WITHOUT ERROR FIXED\n",
    "train_desc = train.Description.fillna(\"none\").values\n",
    "test_desc = test.Description.fillna(\"none\").values\n",
    "\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=10000,\n",
    "        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "        stop_words = 'english')\n",
    "    \n",
    "# Fit TFIDF\n",
    "tfv.fit(list(train_desc))\n",
    "X =  tfv.transform(train_desc)\n",
    "X_test = tfv.transform(test_desc)\n",
    "print(\"X (tfidf):\", X.shape)\n",
    "\n",
    "svd = TruncatedSVD(n_components=120)\n",
    "svd.fit(X)\n",
    "# print(svd.explained_variance_ratio_.sum())\n",
    "# print(svd.explained_variance_ratio_)\n",
    "X = svd.transform(X)\n",
    "print(\"X (svd):\", X.shape)\n",
    "\n",
    "X = pd.DataFrame(X, columns=['svd_{}'.format(i) for i in range(120)])\n",
    "train = pd.concat((train, X), axis=1)\n",
    "X_test = svd.transform(X_test)\n",
    "X_test = pd.DataFrame(X_test, columns=['svd_{}'.format(i) for i in range(120)])\n",
    "test = pd.concat((test, X_test), axis=1)\n",
    "\n",
    "print(\"train:\", train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train - not found:  14720\n",
      "train - not labelled:  0\n",
      "test - not found:  3948\n"
     ]
    }
   ],
   "source": [
    "#load metadata\n",
    "vertex_xs = []\n",
    "vertex_ys = []\n",
    "bounding_confidences = []\n",
    "bounding_importance_fracs = []\n",
    "dominant_blues = []\n",
    "dominant_greens = []\n",
    "dominant_reds = []\n",
    "dominant_pixel_fracs = []\n",
    "dominant_scores = []\n",
    "label_descriptions = []\n",
    "label_scores = []\n",
    "nf_count = 0 #not found\n",
    "nl_count = 0 #not labelled\n",
    "for pet in train_id:\n",
    "    try:\n",
    "        with open('../input/train/train_metadata/' + '-1.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "        vertex_xs.append(vertex_x)\n",
    "        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "        vertex_ys.append(vertex_y)\n",
    "        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "        bounding_confidences.append(bounding_confidence)\n",
    "        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "        bounding_importance_fracs.append(bounding_importance_frac)\n",
    "        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n",
    "        dominant_blues.append(dominant_blue)\n",
    "        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n",
    "        dominant_greens.append(dominant_green)\n",
    "        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n",
    "        dominant_reds.append(dominant_red)\n",
    "        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "        dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "        dominant_scores.append(dominant_score)\n",
    "        if data.get('labelAnnotations'):\n",
    "            label_description = data['labelAnnotations'][0]['description']\n",
    "            label_descriptions.append(label_description)\n",
    "            label_score = data['labelAnnotations'][0]['score']\n",
    "            label_scores.append(label_score)\n",
    "        else:\n",
    "            nl_count += 1\n",
    "            label_descriptions.append('nothing')\n",
    "            label_scores.append(-1)\n",
    "    except FileNotFoundError:\n",
    "        nf_count += 1\n",
    "        vertex_xs.append(-1)\n",
    "        vertex_ys.append(-1)\n",
    "        bounding_confidences.append(-1)\n",
    "        bounding_importance_fracs.append(-1)\n",
    "        dominant_blues.append(-1)\n",
    "        dominant_greens.append(-1)\n",
    "        dominant_reds.append(-1)\n",
    "        dominant_pixel_fracs.append(-1)\n",
    "        dominant_scores.append(-1)\n",
    "        label_descriptions.append('nothing')\n",
    "        label_scores.append(-1)\n",
    "\n",
    "print('train - not found: ', nf_count)\n",
    "print('train - not labelled: ', nl_count)\n",
    "train.loc[:, 'vertex_x'] = vertex_xs\n",
    "train.loc[:, 'vertex_y'] = vertex_ys\n",
    "train.loc[:, 'bounding_confidence'] = bounding_confidences\n",
    "train.loc[:, 'bounding_importance'] = bounding_importance_fracs\n",
    "train.loc[:, 'dominant_blue'] = dominant_blues\n",
    "train.loc[:, 'dominant_green'] = dominant_greens\n",
    "train.loc[:, 'dominant_red'] = dominant_reds\n",
    "train.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "train.loc[:, 'dominant_score'] = dominant_scores\n",
    "train.loc[:, 'label_description'] = label_descriptions\n",
    "train.loc[:, 'label_score'] = label_scores\n",
    "\n",
    "\n",
    "vertex_xs = []\n",
    "vertex_ys = []\n",
    "bounding_confidences = []\n",
    "bounding_importance_fracs = []\n",
    "dominant_blues = []\n",
    "dominant_greens = []\n",
    "dominant_reds = []\n",
    "dominant_pixel_fracs = []\n",
    "dominant_scores = []\n",
    "label_descriptions = []\n",
    "label_scores = []\n",
    "nf_count = 0\n",
    "nl_count = 0\n",
    "for pet in test_id:\n",
    "    try:\n",
    "        with open('../input/test_metadata/' + '-1.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "        vertex_xs.append(vertex_x)\n",
    "        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "        vertex_ys.append(vertex_y)\n",
    "        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "        bounding_confidences.append(bounding_confidence)\n",
    "        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "        bounding_importance_fracs.append(bounding_importance_frac)\n",
    "        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n",
    "        dominant_blues.append(dominant_blue)\n",
    "        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n",
    "        dominant_greens.append(dominant_green)\n",
    "        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n",
    "        dominant_reds.append(dominant_red)\n",
    "        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "        dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "        dominant_scores.append(dominant_score)\n",
    "        if data.get('labelAnnotations'):\n",
    "            label_description = data['labelAnnotations'][0]['description']\n",
    "            label_descriptions.append(label_description)\n",
    "            label_score = data['labelAnnotations'][0]['score']\n",
    "            label_scores.append(label_score)\n",
    "        else:\n",
    "            nl_count += 1\n",
    "            label_descriptions.append('nothing')\n",
    "            label_scores.append(-1)\n",
    "    except FileNotFoundError:\n",
    "        nf_count += 1\n",
    "        vertex_xs.append(-1)\n",
    "        vertex_ys.append(-1)\n",
    "        bounding_confidences.append(-1)\n",
    "        bounding_importance_fracs.append(-1)\n",
    "        dominant_blues.append(-1)\n",
    "        dominant_greens.append(-1)\n",
    "        dominant_reds.append(-1)\n",
    "        dominant_pixel_fracs.append(-1)\n",
    "        dominant_scores.append(-1)\n",
    "        label_descriptions.append('nothing')\n",
    "        label_scores.append(-1)\n",
    "\n",
    "print('test - not found: ', nf_count)\n",
    "test.loc[:, 'vertex_x'] = vertex_xs\n",
    "test.loc[:, 'vertex_y'] = vertex_ys\n",
    "test.loc[:, 'bounding_confidence'] = bounding_confidences\n",
    "test.loc[:, 'bounding_importance'] = bounding_importance_fracs\n",
    "test.loc[:, 'dominant_blue'] = dominant_blues\n",
    "test.loc[:, 'dominant_green'] = dominant_greens\n",
    "test.loc[:, 'dominant_red'] = dominant_reds\n",
    "test.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "test.loc[:, 'dominant_score'] = dominant_scores\n",
    "test.loc[:, 'label_description'] = label_descriptions\n",
    "test.loc[:, 'label_score'] = label_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['Name', 'RescuerID', 'Description'], axis=1, inplace=True)\n",
    "test.drop(['Name', 'RescuerID', 'Description'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14720, 152)\n",
      "(3948, 152)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Age</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Breed2</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>Color3</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>...</th>\n",
       "      <th>vertex_y</th>\n",
       "      <th>bounding_confidence</th>\n",
       "      <th>bounding_importance</th>\n",
       "      <th>dominant_blue</th>\n",
       "      <th>dominant_green</th>\n",
       "      <th>dominant_red</th>\n",
       "      <th>dominant_pixel_frac</th>\n",
       "      <th>dominant_score</th>\n",
       "      <th>label_description</th>\n",
       "      <th>label_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>nothing</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>nothing</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>nothing</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>241</td>\n",
       "      <td>241</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>nothing</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>nothing</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 152 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Type  Age Breed1 Breed2 Gender Color1 Color2 Color3 MaturitySize FurLength  \\\n",
       "0    2    6    265      0      1      4      7      0            2         2   \n",
       "1    1    2    307      0      2      1      2      7            2         2   \n",
       "2    1    2    307      0      2      1      0      0            2         1   \n",
       "3    2    2    241    241      1      1      4      6            2         1   \n",
       "4    1    1    307      0      1      2      5      0            2         1   \n",
       "\n",
       "      ...      vertex_y bounding_confidence bounding_importance dominant_blue  \\\n",
       "0     ...            -1                  -1                  -1            -1   \n",
       "1     ...            -1                  -1                  -1            -1   \n",
       "2     ...            -1                  -1                  -1            -1   \n",
       "3     ...            -1                  -1                  -1            -1   \n",
       "4     ...            -1                  -1                  -1            -1   \n",
       "\n",
       "   dominant_green  dominant_red dominant_pixel_frac  dominant_score  \\\n",
       "0              -1            -1                  -1              -1   \n",
       "1              -1            -1                  -1              -1   \n",
       "2              -1            -1                  -1              -1   \n",
       "3              -1            -1                  -1              -1   \n",
       "4              -1            -1                  -1              -1   \n",
       "\n",
       "   label_description  label_score  \n",
       "0            nothing           -1  \n",
       "1            nothing           -1  \n",
       "2            nothing           -1  \n",
       "3            nothing           -1  \n",
       "4            nothing           -1  \n",
       "\n",
       "[5 rows x 152 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_cols = ['Age', 'Quantity', 'Fee', 'VideoAmt', \n",
    "                'PhotoAmt', 'AdoptionSpeed', 'doc_sent_mag', \n",
    "                'doc_sent_score', 'dominant_score', 'dominant_pixel_frac', \n",
    "                'dominant_red', 'dominant_green', 'dominant_blue', \n",
    "                'bounding_importance', 'bounding_confidence', \n",
    "                'vertex_x', 'vertex_y', 'label_score'] + ['svd_{}'.format(i) for i in range(120)]\n",
    "cat_cols = list(set(train.columns) - set(numeric_cols))\n",
    "train.loc[:, cat_cols] = train[cat_cols].astype('category')\n",
    "test.loc[:, cat_cols] = test[cat_cols].astype('category')\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the categorical features\n",
    "foo = train.dtypes\n",
    "cat_feature_names = foo[foo == \"category\"]\n",
    "cat_features = [train.columns.get_loc(c) for c in train.columns if c in cat_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started lgb fold 1/5\n",
      "Prep LGB\n",
      "Train LGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 16, 150]\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "D:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 1.05922\tvalid_1's rmse: 1.09822\n",
      "[200]\ttraining's rmse: 1.00397\tvalid_1's rmse: 1.07404\n",
      "[300]\ttraining's rmse: 0.969777\tvalid_1's rmse: 1.06407\n",
      "[400]\ttraining's rmse: 0.942187\tvalid_1's rmse: 1.05771\n",
      "[500]\ttraining's rmse: 0.917723\tvalid_1's rmse: 1.05313\n",
      "[600]\ttraining's rmse: 0.89671\tvalid_1's rmse: 1.04985\n",
      "[700]\ttraining's rmse: 0.875032\tvalid_1's rmse: 1.04788\n",
      "[800]\ttraining's rmse: 0.856539\tvalid_1's rmse: 1.04636\n",
      "[900]\ttraining's rmse: 0.839738\tvalid_1's rmse: 1.04521\n",
      "[1000]\ttraining's rmse: 0.824486\tvalid_1's rmse: 1.04483\n",
      "[1100]\ttraining's rmse: 0.808201\tvalid_1's rmse: 1.04428\n",
      "[1200]\ttraining's rmse: 0.791405\tvalid_1's rmse: 1.04437\n",
      "[1300]\ttraining's rmse: 0.774709\tvalid_1's rmse: 1.04373\n",
      "[1400]\ttraining's rmse: 0.759988\tvalid_1's rmse: 1.04347\n",
      "[1500]\ttraining's rmse: 0.745185\tvalid_1's rmse: 1.04326\n",
      "[1600]\ttraining's rmse: 0.73252\tvalid_1's rmse: 1.04306\n",
      "[1700]\ttraining's rmse: 0.719503\tvalid_1's rmse: 1.04286\n",
      "[1800]\ttraining's rmse: 0.707617\tvalid_1's rmse: 1.043\n",
      "Early stopping, best iteration is:\n",
      "[1713]\ttraining's rmse: 0.718435\tvalid_1's rmse: 1.04283\n",
      "Predict 1/2\n",
      "Valid Counts =  Counter({4: 826, 2: 791, 3: 641, 1: 608, 0: 80})\n",
      "Predicted Counts =  Counter({2.0: 1115, 4.0: 865, 3.0: 638, 1.0: 327, 0.0: 1})\n",
      "Coefficients =  [0.51893396 1.88071346 2.47389705 2.80715342]\n",
      "QWK =  0.4251162464003574\n",
      "Predict 2/2\n",
      "lgb cv score 1: RMSE 1.0428256204906576 QWK 0.4251162464003574\n",
      "Started lgb fold 2/5\n",
      "Prep LGB\n",
      "Train LGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 16, 150]\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "D:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 1.05694\tvalid_1's rmse: 1.09955\n",
      "[200]\ttraining's rmse: 1.00184\tvalid_1's rmse: 1.07944\n",
      "[300]\ttraining's rmse: 0.964645\tvalid_1's rmse: 1.07131\n",
      "[400]\ttraining's rmse: 0.935439\tvalid_1's rmse: 1.06684\n",
      "[500]\ttraining's rmse: 0.910514\tvalid_1's rmse: 1.06387\n",
      "[600]\ttraining's rmse: 0.888704\tvalid_1's rmse: 1.06179\n",
      "[700]\ttraining's rmse: 0.867731\tvalid_1's rmse: 1.06019\n",
      "[800]\ttraining's rmse: 0.849182\tvalid_1's rmse: 1.05891\n",
      "[900]\ttraining's rmse: 0.831348\tvalid_1's rmse: 1.05816\n",
      "[1000]\ttraining's rmse: 0.813894\tvalid_1's rmse: 1.05737\n",
      "[1100]\ttraining's rmse: 0.796723\tvalid_1's rmse: 1.05705\n",
      "[1200]\ttraining's rmse: 0.780585\tvalid_1's rmse: 1.05691\n",
      "[1300]\ttraining's rmse: 0.767144\tvalid_1's rmse: 1.05676\n",
      "Early stopping, best iteration is:\n",
      "[1291]\ttraining's rmse: 0.768326\tvalid_1's rmse: 1.05675\n",
      "Predict 1/2\n",
      "Valid Counts =  Counter({4: 826, 2: 791, 3: 641, 1: 608, 0: 80})\n",
      "Predicted Counts =  Counter({2.0: 832, 3.0: 762, 4.0: 684, 1.0: 668})\n",
      "Coefficients =  [0.45194374 2.09224398 2.50316258 2.92517608]\n",
      "QWK =  0.4212923223373207\n",
      "Predict 2/2\n",
      "lgb cv score 2: RMSE 1.0567520895530318 QWK 0.4212923223373207\n",
      "Started lgb fold 3/5\n",
      "Prep LGB\n",
      "Train LGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 16, 150]\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "D:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 1.0583\tvalid_1's rmse: 1.10176\n",
      "[200]\ttraining's rmse: 1.00102\tvalid_1's rmse: 1.07902\n",
      "[300]\ttraining's rmse: 0.964174\tvalid_1's rmse: 1.06932\n",
      "[400]\ttraining's rmse: 0.933181\tvalid_1's rmse: 1.06349\n",
      "[500]\ttraining's rmse: 0.909477\tvalid_1's rmse: 1.06051\n",
      "[600]\ttraining's rmse: 0.887834\tvalid_1's rmse: 1.05797\n",
      "[700]\ttraining's rmse: 0.869165\tvalid_1's rmse: 1.0564\n",
      "[800]\ttraining's rmse: 0.850099\tvalid_1's rmse: 1.05525\n",
      "[900]\ttraining's rmse: 0.833294\tvalid_1's rmse: 1.0549\n",
      "[1000]\ttraining's rmse: 0.817729\tvalid_1's rmse: 1.05414\n",
      "[1100]\ttraining's rmse: 0.801453\tvalid_1's rmse: 1.05341\n",
      "[1200]\ttraining's rmse: 0.786387\tvalid_1's rmse: 1.0533\n",
      "[1300]\ttraining's rmse: 0.77108\tvalid_1's rmse: 1.05267\n",
      "[1400]\ttraining's rmse: 0.756847\tvalid_1's rmse: 1.05246\n",
      "[1500]\ttraining's rmse: 0.741567\tvalid_1's rmse: 1.05193\n",
      "[1600]\ttraining's rmse: 0.7278\tvalid_1's rmse: 1.05156\n",
      "[1700]\ttraining's rmse: 0.714223\tvalid_1's rmse: 1.05154\n",
      "[1800]\ttraining's rmse: 0.701513\tvalid_1's rmse: 1.05149\n",
      "Early stopping, best iteration is:\n",
      "[1760]\ttraining's rmse: 0.707093\tvalid_1's rmse: 1.05128\n",
      "Predict 1/2\n",
      "Valid Counts =  Counter({4: 826, 2: 791, 3: 640, 1: 608, 0: 79})\n",
      "Predicted Counts =  Counter({2.0: 1454, 4.0: 867, 3.0: 412, 1.0: 211})\n",
      "Coefficients =  [0.50168273 1.75757368 2.5709516  2.80357939]\n",
      "QWK =  0.40759511777323687\n",
      "Predict 2/2\n",
      "lgb cv score 3: RMSE 1.0512826507371151 QWK 0.40759511777323687\n",
      "Started lgb fold 4/5\n",
      "Prep LGB\n",
      "Train LGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 16, 150]\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "D:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 1.05862\tvalid_1's rmse: 1.10169\n",
      "[200]\ttraining's rmse: 1.00217\tvalid_1's rmse: 1.07636\n",
      "[300]\ttraining's rmse: 0.966179\tvalid_1's rmse: 1.06404\n",
      "[400]\ttraining's rmse: 0.936003\tvalid_1's rmse: 1.05693\n",
      "[500]\ttraining's rmse: 0.911188\tvalid_1's rmse: 1.05197\n",
      "[600]\ttraining's rmse: 0.889677\tvalid_1's rmse: 1.04864\n",
      "[700]\ttraining's rmse: 0.868674\tvalid_1's rmse: 1.04603\n",
      "[800]\ttraining's rmse: 0.849246\tvalid_1's rmse: 1.044\n",
      "[900]\ttraining's rmse: 0.830157\tvalid_1's rmse: 1.04229\n",
      "[1000]\ttraining's rmse: 0.813294\tvalid_1's rmse: 1.04113\n",
      "[1100]\ttraining's rmse: 0.797461\tvalid_1's rmse: 1.04073\n",
      "[1200]\ttraining's rmse: 0.780641\tvalid_1's rmse: 1.04036\n",
      "[1300]\ttraining's rmse: 0.766668\tvalid_1's rmse: 1.03997\n",
      "[1400]\ttraining's rmse: 0.753837\tvalid_1's rmse: 1.03986\n",
      "[1500]\ttraining's rmse: 0.740412\tvalid_1's rmse: 1.03969\n",
      "Early stopping, best iteration is:\n",
      "[1415]\ttraining's rmse: 0.751081\tvalid_1's rmse: 1.0396\n",
      "Predict 1/2\n",
      "Valid Counts =  Counter({4: 825, 2: 791, 3: 640, 1: 607, 0: 79})\n",
      "Predicted Counts =  Counter({2.0: 1478, 4.0: 739, 3.0: 619, 1.0: 104, 0.0: 2})\n",
      "Coefficients =  [0.52817327 1.68392638 2.51882286 2.8582847 ]\n",
      "QWK =  0.42170020929763496\n",
      "Predict 2/2\n",
      "lgb cv score 4: RMSE 1.0395957221179621 QWK 0.42170020929763496\n",
      "Started lgb fold 5/5\n",
      "Prep LGB\n",
      "Train LGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 16, 150]\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "D:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 1.05844\tvalid_1's rmse: 1.09772\n",
      "[200]\ttraining's rmse: 1.0019\tvalid_1's rmse: 1.07285\n",
      "[300]\ttraining's rmse: 0.966038\tvalid_1's rmse: 1.06357\n",
      "[400]\ttraining's rmse: 0.936489\tvalid_1's rmse: 1.05817\n",
      "[500]\ttraining's rmse: 0.91168\tvalid_1's rmse: 1.05461\n",
      "[600]\ttraining's rmse: 0.886319\tvalid_1's rmse: 1.05183\n",
      "[700]\ttraining's rmse: 0.864628\tvalid_1's rmse: 1.04956\n",
      "[800]\ttraining's rmse: 0.844688\tvalid_1's rmse: 1.04766\n",
      "[900]\ttraining's rmse: 0.825881\tvalid_1's rmse: 1.04658\n",
      "[1000]\ttraining's rmse: 0.808867\tvalid_1's rmse: 1.04543\n",
      "[1100]\ttraining's rmse: 0.791425\tvalid_1's rmse: 1.04446\n",
      "[1200]\ttraining's rmse: 0.775055\tvalid_1's rmse: 1.0437\n",
      "[1300]\ttraining's rmse: 0.757944\tvalid_1's rmse: 1.0431\n",
      "[1400]\ttraining's rmse: 0.741408\tvalid_1's rmse: 1.04224\n",
      "[1500]\ttraining's rmse: 0.726465\tvalid_1's rmse: 1.04162\n",
      "[1600]\ttraining's rmse: 0.712155\tvalid_1's rmse: 1.04133\n",
      "[1700]\ttraining's rmse: 0.697742\tvalid_1's rmse: 1.04114\n",
      "[1800]\ttraining's rmse: 0.684149\tvalid_1's rmse: 1.04097\n",
      "Early stopping, best iteration is:\n",
      "[1793]\ttraining's rmse: 0.685256\tvalid_1's rmse: 1.0408\n",
      "Predict 1/2\n",
      "Valid Counts =  Counter({4: 825, 2: 791, 3: 640, 1: 607, 0: 79})\n",
      "Predicted Counts =  Counter({2.0: 1475, 4.0: 801, 3.0: 502, 1.0: 163, 0.0: 1})\n",
      "Coefficients =  [0.51779464 1.69384953 2.57507346 2.86349608]\n",
      "QWK =  0.41241364058533925\n",
      "Predict 2/2\n",
      "lgb cv score 5: RMSE 1.0408025271584715 QWK 0.41241364058533925\n",
      "lgb cv RMSE scores : [1.0428256204906576, 1.0567520895530318, 1.0512826507371151, 1.0395957221179621, 1.0408025271584715]\n",
      "lgb cv mean RMSE score : 1.0462517220114478\n",
      "lgb cv std RMSE score : 1.0462517220114478\n",
      "lgb cv QWK scores : [0.4251162464003574, 0.4212923223373207, 0.40759511777323687, 0.42170020929763496, 0.41241364058533925]\n",
      "lgb cv mean QWK score : 0.41762350727877784\n",
      "lgb cv std QWK score : 0.006541133691748526\n"
     ]
    }
   ],
   "source": [
    "def run_cv_model(train, test, target, model_fn, params={}, eval_fn=None, label='model'):\n",
    "    kf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    fold_splits = kf.split(train, target)\n",
    "    cv_scores = []\n",
    "    qwk_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros((train.shape[0], 5))\n",
    "    all_coefficients = np.zeros((5, 4))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    i = 1\n",
    "    for dev_index, val_index in fold_splits:\n",
    "        print('Started ' + label + ' fold ' + str(i) + '/5')\n",
    "        if isinstance(train, pd.DataFrame):\n",
    "            dev_X, val_X = train.iloc[dev_index], train.iloc[val_index]\n",
    "            dev_y, val_y = target[dev_index], target[val_index]\n",
    "        else:\n",
    "            dev_X, val_X = train[dev_index], train[val_index]\n",
    "            dev_y, val_y = target[dev_index], target[val_index]\n",
    "        params2 = params.copy()\n",
    "        pred_val_y, pred_test_y, importances, coefficients, qwk = model_fn(dev_X, dev_y, val_X, val_y, test, params2)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index] = pred_val_y\n",
    "        all_coefficients[i-1, :] = coefficients\n",
    "        if eval_fn is not None:\n",
    "            cv_score = eval_fn(val_y, pred_val_y)\n",
    "            cv_scores.append(cv_score)\n",
    "            qwk_scores.append(qwk)\n",
    "            print(label + ' cv score {}: RMSE {} QWK {}'.format(i, cv_score, qwk))\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature'] = train.columns.values\n",
    "        fold_importance_df['importance'] = importances\n",
    "        fold_importance_df['fold'] = i\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)        \n",
    "        i += 1\n",
    "    print('{} cv RMSE scores : {}'.format(label, cv_scores))\n",
    "    print('{} cv mean RMSE score : {}'.format(label, np.mean(cv_scores)))\n",
    "    print('{} cv std RMSE score : {}'.format(label, np.mean(cv_scores)))\n",
    "    print('{} cv QWK scores : {}'.format(label, qwk_scores))\n",
    "    print('{} cv mean QWK score : {}'.format(label, np.mean(qwk_scores)))\n",
    "    print('{} cv std QWK score : {}'.format(label, np.std(qwk_scores)))\n",
    "    pred_full_test = pred_full_test / 5.0\n",
    "    results = {'label': label,\n",
    "               'train': pred_train, 'test': pred_full_test,\n",
    "                'cv': cv_scores, 'qwk': qwk_scores,\n",
    "               'importance': feature_importance_df,\n",
    "               'coefficients': all_coefficients}\n",
    "    return results\n",
    "\n",
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 70,\n",
    "          'max_depth': 8,\n",
    "          'learning_rate': 0.01,\n",
    "          'bagging_fraction': 0.85,\n",
    "          'feature_fraction': 0.8,\n",
    "          'min_split_gain': 0.02,\n",
    "          'min_child_samples': 150,\n",
    "          'min_child_weight': 0.02,\n",
    "          'lambda_l2': 0.0475,\n",
    "          'verbosity': -1,\n",
    "          'data_random_seed': 17,\n",
    "          'early_stop': 100,\n",
    "          'verbose_eval': 100,\n",
    "          'num_rounds': 10000}\n",
    "\n",
    "def runLGB(train_X, train_y, test_X, test_y, test_X2, params):\n",
    "    print('Prep LGB')\n",
    "    d_train = lgb.Dataset(train_X, label=train_y)\n",
    "    d_valid = lgb.Dataset(test_X, label=test_y)\n",
    "    watchlist = [d_train, d_valid]\n",
    "    print('Train LGB')\n",
    "    num_rounds = params.pop('num_rounds')\n",
    "    verbose_eval = params.pop('verbose_eval')\n",
    "    early_stop = None\n",
    "    if params.get('early_stop'):\n",
    "        early_stop = params.pop('early_stop')\n",
    "    model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=num_rounds,\n",
    "                      valid_sets=watchlist,\n",
    "                      verbose_eval=verbose_eval,\n",
    "                      categorical_feature=list(cat_features),\n",
    "                      early_stopping_rounds=early_stop)\n",
    "    \n",
    "    print('Predict 1/2')\n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    optR = OptimizedRounder()\n",
    "    optR.fit(pred_test_y, test_y)\n",
    "    coefficients = optR.coefficients()\n",
    "    pred_test_y_k = optR.predict(pred_test_y, coefficients)\n",
    "    print(\"Valid Counts = \", Counter(test_y))\n",
    "    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "    print(\"Coefficients = \", coefficients)\n",
    "    qwk = quadratic_weighted_kappa(test_y, pred_test_y_k)\n",
    "    print(\"QWK = \", qwk)\n",
    "    print('Predict 2/2')\n",
    "    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n",
    "    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), model.feature_importance(), coefficients, qwk\n",
    "\n",
    "results = run_cv_model(train, test, target, runLGB, params, rmse, 'lgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breed1</td>\n",
       "      <td>1898.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Age</td>\n",
       "      <td>992.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Breed2</td>\n",
       "      <td>705.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PhotoAmt</td>\n",
       "      <td>566.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>State</td>\n",
       "      <td>554.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>svd_46</td>\n",
       "      <td>493.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>svd_53</td>\n",
       "      <td>470.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>svd_40</td>\n",
       "      <td>439.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Quantity</td>\n",
       "      <td>436.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>svd_0</td>\n",
       "      <td>431.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>svd_4</td>\n",
       "      <td>424.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>svd_91</td>\n",
       "      <td>417.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>svd_69</td>\n",
       "      <td>417.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>svd_109</td>\n",
       "      <td>416.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>svd_78</td>\n",
       "      <td>416.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>svd_71</td>\n",
       "      <td>413.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>svd_119</td>\n",
       "      <td>410.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>svd_77</td>\n",
       "      <td>408.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>svd_101</td>\n",
       "      <td>406.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>svd_15</td>\n",
       "      <td>404.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>svd_21</td>\n",
       "      <td>395.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>svd_114</td>\n",
       "      <td>394.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>svd_24</td>\n",
       "      <td>393.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>svd_12</td>\n",
       "      <td>390.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>svd_100</td>\n",
       "      <td>386.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>svd_30</td>\n",
       "      <td>386.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>svd_8</td>\n",
       "      <td>385.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>svd_65</td>\n",
       "      <td>383.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>svd_58</td>\n",
       "      <td>382.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>svd_75</td>\n",
       "      <td>380.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>svd_11</td>\n",
       "      <td>243.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>svd_60</td>\n",
       "      <td>242.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>svd_80</td>\n",
       "      <td>242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>svd_14</td>\n",
       "      <td>241.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>svd_79</td>\n",
       "      <td>238.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>svd_57</td>\n",
       "      <td>238.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>svd_50</td>\n",
       "      <td>231.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sterilized</td>\n",
       "      <td>212.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FurLength</td>\n",
       "      <td>212.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Gender</td>\n",
       "      <td>188.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MaturitySize</td>\n",
       "      <td>174.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Vaccinated</td>\n",
       "      <td>128.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dewormed</td>\n",
       "      <td>114.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Color3</td>\n",
       "      <td>91.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Health</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Type</td>\n",
       "      <td>32.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>VideoAmt</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>vertex_x</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>dominant_score</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bounding_confidence</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>bounding_importance</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>doc_sent_mag</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>doc_sent_score</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>dominant_blue</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>dominant_green</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>dominant_pixel_frac</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>dominant_red</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>label_description</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>label_score</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>vertex_y</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 feature  importance\n",
       "1                 Breed1      1898.8\n",
       "0                    Age       992.6\n",
       "2                 Breed2       705.6\n",
       "12              PhotoAmt       566.4\n",
       "14                 State       554.0\n",
       "91                svd_46       493.0\n",
       "99                svd_53       470.4\n",
       "85                svd_40       439.8\n",
       "13              Quantity       436.4\n",
       "30                 svd_0       431.8\n",
       "84                 svd_4       424.6\n",
       "141               svd_91       417.8\n",
       "116               svd_69       417.0\n",
       "42               svd_109       416.6\n",
       "126               svd_78       416.4\n",
       "119               svd_71       413.6\n",
       "53               svd_119       410.8\n",
       "125               svd_77       408.2\n",
       "34               svd_101       406.0\n",
       "57                svd_15       404.0\n",
       "64                svd_21       395.8\n",
       "48               svd_114       394.4\n",
       "67                svd_24       393.6\n",
       "54                svd_12       390.2\n",
       "33               svd_100       386.6\n",
       "74                svd_30       386.0\n",
       "128                svd_8       385.2\n",
       "112               svd_65       383.4\n",
       "104               svd_58       382.4\n",
       "123               svd_75       380.2\n",
       "..                   ...         ...\n",
       "43                svd_11       243.6\n",
       "107               svd_60       242.4\n",
       "129               svd_80       242.0\n",
       "56                svd_14       241.4\n",
       "127               svd_79       238.2\n",
       "103               svd_57       238.0\n",
       "96                svd_50       231.4\n",
       "15            Sterilized       212.8\n",
       "8              FurLength       212.4\n",
       "9                 Gender       188.2\n",
       "11          MaturitySize       174.4\n",
       "17            Vaccinated       128.4\n",
       "6               Dewormed       114.6\n",
       "5                 Color3        91.2\n",
       "10                Health        76.0\n",
       "16                  Type        32.8\n",
       "18              VideoAmt         5.8\n",
       "150             vertex_x         0.0\n",
       "27        dominant_score         0.0\n",
       "19   bounding_confidence         0.0\n",
       "20   bounding_importance         0.0\n",
       "21          doc_sent_mag         0.0\n",
       "22        doc_sent_score         0.0\n",
       "23         dominant_blue         0.0\n",
       "24        dominant_green         0.0\n",
       "25   dominant_pixel_frac         0.0\n",
       "26          dominant_red         0.0\n",
       "28     label_description         0.0\n",
       "29           label_score         0.0\n",
       "151             vertex_y         0.0\n",
       "\n",
       "[152 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imports = results['importance'].groupby('feature')['feature', 'importance'].mean().reset_index()\n",
    "imports.sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50370567 1.82166141 2.52838151 2.85153794]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({2: 4368, 1: 2903, 3: 2910, 4: 3957, 0: 582})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optR = OptimizedRounder()\n",
    "coefficients_ = np.mean(results['coefficients'], axis=0)\n",
    "print(coefficients_)\n",
    "# manually adjust coefs\n",
    "coefficients_[0] = 1.64\n",
    "coefficients_[1] = 2.11\n",
    "coefficients_[3] = 2.85\n",
    "train_predictions = [r[0] for r in results['train']]\n",
    "train_predictions = optR.predict(train_predictions, coefficients_).astype(int)\n",
    "Counter(train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50370567 1.82166141 2.52838151 2.85153794]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({2: 1175, 4: 1120, 3: 766, 1: 803, 0: 84})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optR = OptimizedRounder()\n",
    "coefficients_ = np.mean(results['coefficients'], axis=0)\n",
    "print(coefficients_)\n",
    "# manually adjust coefs\n",
    "coefficients_[0] = 1.645\n",
    "coefficients_[1] = 2.115\n",
    "coefficients_[3] = 2.84\n",
    "test_predictions = [r[0] for r in results['test']]\n",
    "test_predictions = optR.predict(test_predictions, coefficients_).astype(int)\n",
    "Counter(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Distribution:\n",
      "0    0.026970\n",
      "1    0.206386\n",
      "2    0.268682\n",
      "3    0.217527\n",
      "4    0.280435\n",
      "Name: AdoptionSpeed, dtype: float64\n",
      "Test Predicted Distribution:\n",
      "0    0.021277\n",
      "1    0.203394\n",
      "2    0.297619\n",
      "3    0.194022\n",
      "4    0.283688\n",
      "dtype: float64\n",
      "Train Predicted Distribution:\n",
      "0    0.039538\n",
      "1    0.197215\n",
      "2    0.296739\n",
      "3    0.197690\n",
      "4    0.268818\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"True Distribution:\")\n",
    "print(pd.value_counts(target, normalize=True).sort_index())\n",
    "print(\"Test Predicted Distribution:\")\n",
    "print(pd.value_counts(test_predictions, normalize=True).sort_index())\n",
    "print(\"Train Predicted Distribution:\")\n",
    "print(pd.value_counts(train_predictions, normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>118</td>\n",
       "      <td>103</td>\n",
       "      <td>49</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>279</td>\n",
       "      <td>1023</td>\n",
       "      <td>995</td>\n",
       "      <td>463</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150</td>\n",
       "      <td>988</td>\n",
       "      <td>1436</td>\n",
       "      <td>736</td>\n",
       "      <td>645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68</td>\n",
       "      <td>533</td>\n",
       "      <td>1073</td>\n",
       "      <td>786</td>\n",
       "      <td>742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>241</td>\n",
       "      <td>761</td>\n",
       "      <td>876</td>\n",
       "      <td>2230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4\n",
       "0   65   118   103   49    62\n",
       "1  279  1023   995  463   278\n",
       "2  150   988  1436  736   645\n",
       "3   68   533  1073  786   742\n",
       "4   20   241   761  876  2230"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(sk_cmatrix(target, train_predictions), index=list(range(5)), columns=list(range(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PetID</th>\n",
       "      <th>AdoptionSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>378fcc4fc</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73c10e136</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72000c4c5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e147a4b9f</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43fbba852</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PetID  AdoptionSpeed\n",
       "0  378fcc4fc              2\n",
       "1  73c10e136              4\n",
       "2  72000c4c5              4\n",
       "3  e147a4b9f              3\n",
       "4  43fbba852              4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quadratic_weighted_kappa(target, train_predictions)\n",
    "rmse(target, [r[0] for r in results['train']])\n",
    "submission = pd.DataFrame({'PetID': test_id, 'AdoptionSpeed': test_predictions})\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK =  0.41241364058533925\n"
     ]
    }
   ],
   "source": [
    "print(\"QWK =  0.41241364058533925\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
