{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from https://www.kaggle.com/myltykritik/simple-lgbm-image-features\n",
    "import json\n",
    "\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import lightgbm as lgb\n",
    "np.random.seed(369)\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following 3 functions have been taken from Ben Hamner's github repository\n",
    "# https://github.com/benhamner/Metrics\n",
    "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = y\n",
    "    rater_b = y_pred\n",
    "    min_rating=None\n",
    "    max_rating=None\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return (1.0 - numerator / denominator)\n",
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        ll = quadratic_weighted_kappa(y, X_p)\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']\n",
    "    \n",
    "def rmse(actual, predicted):\n",
    "    return sqrt(mean_squared_error(actual, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Features ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from keras.applications.densenet import preprocess_input, DenseNet121\n",
    "import tensorflow as tf\n",
    "\n",
    "train_df = pd.read_csv('input/train/train.csv')\n",
    "img_size = 256\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pet_ids = train_df['PetID'].values\n",
    "n_batches = len(pet_ids) // batch_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_to_square(im):\n",
    "    old_size = im.shape[:2] # old_size is in (height, width) format\n",
    "    ratio = float(img_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    # new_size should be in (width, height) format\n",
    "    im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "    delta_w = img_size - new_size[1]\n",
    "    delta_h = img_size - new_size[0]\n",
    "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "    color = [0, 0, 0]\n",
    "    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n",
    "    return new_im\n",
    "\n",
    "def load_image(path, pet_id):\n",
    "    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n",
    "    new_image = resize_to_square(image)\n",
    "    new_image = preprocess_input(new_image)\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "\n",
    "def train_keras():\n",
    "    from keras.models import Model\n",
    "    from keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "\n",
    "    #https://regressionsessionsblog.wordpress.com/2018/06/11/stuck-on-an-issue-making-keras-predictions-in-parallel/\n",
    "    #https://github.com/keras-team/keras/issues/4740\n",
    "    jobs = 6\n",
    "    config = tf.ConfigProto(intra_op_parallelism_threads=jobs, \\\n",
    "                            inter_op_parallelism_threads=jobs, \\\n",
    "                            allow_soft_placement=True, \\\n",
    "                            device_count = {'CPU': jobs})\n",
    "    session = tf.Session(config=config)\n",
    "    K.set_session(session)\n",
    "\n",
    "    print(\"start\")\n",
    "    inp = Input((256,256,3))\n",
    "    backbone = DenseNet121(input_tensor = inp, \n",
    "                            weights=\"input/densenet-keras/DenseNet-BC-121-32-no-top.h5\",\n",
    "                            include_top = False)\n",
    "    x = backbone.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\n",
    "    x = AveragePooling1D(4)(x)\n",
    "    out = Lambda(lambda x: x[:,:,0])(x)\n",
    "\n",
    "    m = Model(inp,out)\n",
    "    print(\"done\")\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"start keras prediction\")\n",
    "\n",
    "    for b in tqdm_notebook(range(n_batches)):\n",
    "        start = b*batch_size\n",
    "        end = (b+1)*batch_size\n",
    "        batch_pets = pet_ids[start:end]\n",
    "        batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            try:\n",
    "                batch_images[i] = load_image(\"input/train/train_images/\", pet_id)\n",
    "            except:\n",
    "                pass\n",
    "        batch_preds = m.predict(batch_images)\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            features[pet_id] = batch_preds[i]\n",
    "    print(\"done keras prediction\")\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "done\n",
      "start keras prediction\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e67e192f3af4941a9a55ca75f1ae3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=938), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done keras prediction\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "m = train_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(features, filename=\"train_features\"):\n",
    "    with open(filename + \".json\", 'w') as file:\n",
    "        json.dump(features,file)\n",
    "        #json.dump(dictionary, file, sort_keys=True, indent=4)\n",
    "    return\n",
    "def load_json(filename=\"file\", dictionary=features):\n",
    "    with open(filename + \".json\", 'r') as file:\n",
    "        loaded_file = json.load(file)\n",
    "    return loaded_file\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,\n",
    "            np.int16, np.int32, np.int64, np.uint8,\n",
    "            np.uint16, np.uint32, np.uint64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.float_, np.float16, np.float32, \n",
    "            np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj,(np.ndarray,)): #### This is the fix\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "dumped = json.dumps(features, cls=NumpyEncoder)\n",
    "\n",
    "with open(\"train_features.json\", 'w') as f:\n",
    "    json.dump(dumped, f)\n",
    "    print(\"Saved train features json to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model weights to disk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(m.summary())\n",
    "\n",
    "m.save_weights(\"keras_model_weights.h5\")\n",
    "print(\"Saved model weights to disk\")\n",
    "#with open(\"train_features.json\", 'r') as f:\n",
    "#    train_loaded_features = json.load(f)\n",
    "#    train_loaded_features = json.loads(train_loaded_features)\n",
    "#    print(\"loaded train features\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start create model\n",
      "done\n",
      "start keras test prediction\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087b36f307d34e90a643ec0bb94c3825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=247), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done keras test prediction\n",
      "Saved test features json to disk\n"
     ]
    }
   ],
   "source": [
    "train_feats = pd.DataFrame.from_dict(features, orient='index')\n",
    "train_feats.columns = ['pic_'+str(i) for i in range(train_feats.shape[1])]\n",
    "test_df = pd.read_csv('input/test/test.csv')\n",
    "\n",
    "pet_ids = test_df['PetID'].values\n",
    "n_batches = len(pet_ids) // batch_size + 1\n",
    "test_features = {}\n",
    "\n",
    "def train_test_keras(m, n_batches):\n",
    "    from keras.models import Model\n",
    "    from keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "\n",
    "    #https://regressionsessionsblog.wordpress.com/2018/06/11/stuck-on-an-issue-making-keras-predictions-in-parallel/\n",
    "    #https://github.com/keras-team/keras/issues/4740\n",
    "    jobs = 6\n",
    "    config = tf.ConfigProto(intra_op_parallelism_threads=jobs, \\\n",
    "                            inter_op_parallelism_threads=jobs, \\\n",
    "                            allow_soft_placement=True, \\\n",
    "                            device_count = {'CPU': jobs})\n",
    "    session = tf.Session(config=config)\n",
    "    K.set_session(session)\n",
    "\n",
    "    print(\"start create model\")\n",
    "    inp = Input((256,256,3))\n",
    "    backbone = DenseNet121(input_tensor = inp, \n",
    "                            weights=\"input/densenet-keras/DenseNet-BC-121-32-no-top.h5\",\n",
    "                            include_top = False)\n",
    "    x = backbone.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\n",
    "    x = AveragePooling1D(4)(x)\n",
    "    out = Lambda(lambda x: x[:,:,0])(x)\n",
    "\n",
    "    m = Model(inp,out)\n",
    "    print(\"done\")\n",
    "    \n",
    "    print(\"start keras test prediction\")\n",
    "\n",
    "    for b in tqdm_notebook(range(n_batches)):\n",
    "        start = b*batch_size\n",
    "        end = (b+1)*batch_size\n",
    "        batch_pets = pet_ids[start:end]\n",
    "        batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            try:\n",
    "                batch_images[i] = load_image(\"input/test/test_images/\", pet_id)\n",
    "            except:\n",
    "                pass\n",
    "        batch_preds = m.predict(batch_images)\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            test_features[pet_id] = batch_preds[i]\n",
    "    print(\"done keras test prediction\")\n",
    "    return\n",
    "\n",
    "train_test_keras(m, n_batches)\n",
    "\n",
    "dumped = json.dumps(test_features, cls=NumpyEncoder)\n",
    "\n",
    "with open(\"test_features.json\", 'w') as f:\n",
    "    json.dump(dumped, f)\n",
    "    print(\"Saved test features json to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved image test features prediction values to CSV\n"
     ]
    }
   ],
   "source": [
    "test_feats = pd.DataFrame.from_dict(features, orient='index')\n",
    "test_feats.columns = ['pic_'+str(i) for i in range(test_feats.shape[1])]\n",
    "test_feats = test_feats.reset_index()\n",
    "test_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n",
    "\n",
    "train_feats = train_feats.reset_index()\n",
    "train_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n",
    "\n",
    "test_feats.head()\n",
    "\n",
    "test_feats.to_csv(r'csv_out/img_test_feats_prediction_values.csv')\n",
    "print(\"saved image test features prediction values to CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Here after image prediction files have been made ### \n",
    "\n",
    "this uses train and test.csv files along with the test_features and train_features json files, from which you will be able to run the remainder of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded train features\n",
      "loaded test features\n",
      "creating dataframes from feature dictionaries\n",
      "joining dataframes\n",
      "saved train csv image merge\n",
      "saved test csv image merge\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"input/train/train.csv\")\n",
    "test = pd.read_csv(\"input/test/test.csv\")\n",
    "\n",
    "target = train['AdoptionSpeed']\n",
    "train_id = train['PetID']\n",
    "test_id = test['PetID']\n",
    "\n",
    "loaded_train_feats = {}\n",
    "loaded_test_feats = {}\n",
    "with open(\"train_features.json\", 'r') as f:\n",
    "    loaded_train_feats = json.load(f)\n",
    "    loaded_train_feats = json.loads(loaded_train_feats)\n",
    "    print(\"loaded train features\")\n",
    "\n",
    "with open(\"test_features.json\", 'r') as f:\n",
    "    loaded_test_feats = json.load(f)\n",
    "    loaded_test_feats = json.loads(loaded_test_feats)\n",
    "    print(\"loaded test features\") \n",
    "\n",
    "print(\"creating dataframes from feature dictionaries\")\n",
    "train_feats = pd.DataFrame.from_dict(loaded_train_feats, orient='index')\n",
    "train_feats.columns = ['pic_'+str(i) for i in range(train_feats.shape[1])]\n",
    "\n",
    "test_feats = pd.DataFrame.from_dict(loaded_test_feats, orient='index')\n",
    "test_feats.columns = ['pic_'+str(i) for i in range(test_feats.shape[1])]\n",
    "\n",
    "train_feats = train_feats.reset_index()\n",
    "train_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n",
    "test_feats = test_feats.reset_index()\n",
    "test_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n",
    "\n",
    "print(\"joining dataframes\")\n",
    "train = train.join(train_feats.set_index('PetID'),on='PetID')\n",
    "test = test.join(test_feats.set_index('PetID'),on='PetID')\n",
    "#train = pd.merge(train, train_feats, on = ['PetID'], how = 'left')\n",
    "#test = pd.merge(test, test_feats, left_on = ['PetID'], right_on = ['PetID'], how = 'outer')\n",
    "\n",
    "train.drop(['AdoptionSpeed', 'PetID'], axis=1, inplace=True)\n",
    "test.drop(['PetID'], axis=1, inplace=True)\n",
    "\n",
    "#prepared = train\n",
    "#prepared.drop(['RescuerID', 'Name', 'Description', 'Vaccinated', \n",
    "#               'Color3', 'Dewormed', 'Health', 'VideoAmt', 'Type'], axis=1, inplace=True)\n",
    "\n",
    "#train.drop(['RescuerID', 'Name', 'Description', 'Vaccinated', \n",
    "#               'Color3', 'Dewormed', 'Health', 'VideoAmt', 'Type'], axis=1, inplace=True)\n",
    "#test.drop(['RescuerID', 'Name', 'Description', 'Vaccinated', \n",
    "#               'Color3', 'Dewormed', 'Health', 'VideoAmt', 'Type'], axis=1, inplace=True)\n",
    "\n",
    "#prepared.to_csv(r'csv_out/cleaned_img_merge.csv')\n",
    "#print(\"saved prepared csv\")\n",
    "train.to_csv(r'csv_out/train_csv_img_merge.csv')\n",
    "print(\"saved train csv image merge\")\n",
    "test.to_csv(r'csv_out/test_csv_img_merge.csv')\n",
    "print(\"saved test csv image merge\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading sentiment train\n",
      "loading sentiment test\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "doc_sent_mag = []\n",
    "doc_sent_score = []\n",
    "nf_count = 0\n",
    "print(\"loading sentiment train\")\n",
    "for petid in train_id:\n",
    "    try:\n",
    "        with open('input/train/train_sentiment/' + petid + '.json', 'r') as f:\n",
    "            sentiment = json.load(f)\n",
    "        doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "        doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "    except  Exception:\n",
    "        nf_count += 1\n",
    "        doc_sent_mag.append(-1)\n",
    "        doc_sent_score.append(-1)\n",
    "        pass\n",
    "\n",
    "train.loc[:, 'doc_sent_mag'] = doc_sent_mag\n",
    "train.loc[:, 'doc_sent_score'] = doc_sent_score\n",
    "\n",
    "doc_sent_mag = []\n",
    "doc_sent_score = []\n",
    "nf_count = 0\n",
    "print(\"loading sentiment test\")\n",
    "for petid in test_id:\n",
    "    try:\n",
    "        with open('input/test/test_sentiment/' + petid + '.json', 'r') as f:\n",
    "            sentiment = json.load(f)\n",
    "        doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "        doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "    except Exception:\n",
    "        nf_count += 1\n",
    "        doc_sent_mag.append(-1)\n",
    "        doc_sent_score.append(-1)\n",
    "        pass\n",
    "\n",
    "test.loc[:, 'doc_sent_mag'] = doc_sent_mag\n",
    "test.loc[:, 'doc_sent_score'] = doc_sent_score\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit training sentiment descriptions\n",
      "X (tfidf): (14993, 10000)\n",
      "X (svd): (14993, 200)\n",
      "train: (14993, 480)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"scikit training sentiment descriptions\")\n",
    "## WITHOUT ERROR FIXED\n",
    "train_desc = train.Description.fillna(\"none\").values\n",
    "test_desc = test.Description.fillna(\"none\").values\n",
    "\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=10000,\n",
    "        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "        stop_words = 'english')\n",
    "    \n",
    "# Fit TFIDF\n",
    "tfv.fit(list(train_desc))\n",
    "X =  tfv.transform(train_desc)\n",
    "X_test = tfv.transform(test_desc)\n",
    "print(\"X (tfidf):\", X.shape)\n",
    "\n",
    "svd = TruncatedSVD(n_components=200)\n",
    "svd.fit(X)\n",
    "# print(svd.explained_variance_ratio_.sum())\n",
    "# print(svd.explained_variance_ratio_)\n",
    "X = svd.transform(X)\n",
    "print(\"X (svd):\", X.shape)\n",
    "\n",
    "X = pd.DataFrame(X, columns=['svd_{}'.format(i) for i in range(200)])\n",
    "train = pd.concat((train, X), axis=1)\n",
    "X_test = svd.transform(X_test)\n",
    "X_test = pd.DataFrame(X_test, columns=['svd_{}'.format(i) for i in range(200)])\n",
    "test = pd.concat((test, X_test), axis=1)\n",
    "\n",
    "print(\"train:\", train.shape)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading metadata train\n",
      "370\n",
      "2\n",
      "loading metadata test\n",
      "137\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "vertex_xs = []\n",
    "vertex_ys = []\n",
    "bounding_confidences = []\n",
    "bounding_importance_fracs = []\n",
    "dominant_blues = []\n",
    "dominant_greens = []\n",
    "dominant_reds = []\n",
    "dominant_pixel_fracs = []\n",
    "dominant_scores = []\n",
    "label_descriptions = []\n",
    "label_scores = []\n",
    "nf_count = 0\n",
    "nl_count = 0\n",
    "print(\"loading metadata train\")\n",
    "for petid in train_id:\n",
    "    try:\n",
    "        with open('input/train/train_metadata/' + petid + '-1.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "            vertex_xs.append(vertex_x)\n",
    "            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "            vertex_ys.append(vertex_y)\n",
    "            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "            bounding_confidences.append(bounding_confidence)\n",
    "            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "            bounding_importance_fracs.append(bounding_importance_frac)\n",
    "            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n",
    "            dominant_blues.append(dominant_blue)\n",
    "            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n",
    "            dominant_greens.append(dominant_green)\n",
    "            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n",
    "            dominant_reds.append(dominant_red)\n",
    "            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "            dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "            dominant_scores.append(dominant_score)\n",
    "            if data.get('labelAnnotations'):\n",
    "                label_description = data['labelAnnotations'][0]['description']\n",
    "                label_descriptions.append(label_description)\n",
    "                label_score = data['labelAnnotations'][0]['score']\n",
    "                label_scores.append(label_score)\n",
    "            else:\n",
    "                nl_count += 1\n",
    "                label_descriptions.append('nothing')\n",
    "                label_scores.append(-1)\n",
    "    except Exception:\n",
    "        nf_count += 1\n",
    "        vertex_xs.append(-1)\n",
    "        vertex_ys.append(-1)\n",
    "        bounding_confidences.append(-1)\n",
    "        bounding_importance_fracs.append(-1)\n",
    "        dominant_blues.append(-1)\n",
    "        dominant_greens.append(-1)\n",
    "        dominant_reds.append(-1)\n",
    "        dominant_pixel_fracs.append(-1)\n",
    "        dominant_scores.append(-1)\n",
    "        label_descriptions.append('nothing')\n",
    "        label_scores.append(-1)\n",
    "        pass\n",
    "\n",
    "print(nf_count)\n",
    "print(nl_count)\n",
    "train.loc[:, 'vertex_x'] = vertex_xs\n",
    "train.loc[:, 'vertex_y'] = vertex_ys\n",
    "train.loc[:, 'bounding_confidence'] = bounding_confidences\n",
    "train.loc[:, 'bounding_importance'] = bounding_importance_fracs\n",
    "train.loc[:, 'dominant_blue'] = dominant_blues\n",
    "train.loc[:, 'dominant_green'] = dominant_greens\n",
    "train.loc[:, 'dominant_red'] = dominant_reds\n",
    "train.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "train.loc[:, 'dominant_score'] = dominant_scores\n",
    "train.loc[:, 'label_description'] = label_descriptions\n",
    "train.loc[:, 'label_score'] = label_scores\n",
    "\n",
    "\n",
    "vertex_xs = []\n",
    "vertex_ys = []\n",
    "bounding_confidences = []\n",
    "bounding_importance_fracs = []\n",
    "dominant_blues = []\n",
    "dominant_greens = []\n",
    "dominant_reds = []\n",
    "dominant_pixel_fracs = []\n",
    "dominant_scores = []\n",
    "label_descriptions = []\n",
    "label_scores = []\n",
    "nf_count = 0\n",
    "nl_count = 0\n",
    "print(\"loading metadata test\")\n",
    "for petid in test_id:\n",
    "    try:\n",
    "        with open('input/test/test_metadata/' + petid + '-1.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "            vertex_xs.append(vertex_x)\n",
    "            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "            vertex_ys.append(vertex_y)\n",
    "            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "            bounding_confidences.append(bounding_confidence)\n",
    "            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "            bounding_importance_fracs.append(bounding_importance_frac)\n",
    "            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n",
    "            dominant_blues.append(dominant_blue)\n",
    "            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n",
    "            dominant_greens.append(dominant_green)\n",
    "            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n",
    "            dominant_reds.append(dominant_red)\n",
    "            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "            dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "            dominant_scores.append(dominant_score)\n",
    "            if data.get('labelAnnotations'):\n",
    "                label_description = data['labelAnnotations'][0]['description']\n",
    "                label_descriptions.append(label_description)\n",
    "                label_score = data['labelAnnotations'][0]['score']\n",
    "                label_scores.append(label_score)\n",
    "            else:\n",
    "                nl_count += 1\n",
    "                label_descriptions.append('nothing')\n",
    "                label_scores.append(-1)\n",
    "    except Exception:\n",
    "        nf_count += 1\n",
    "        vertex_xs.append(-1)\n",
    "        vertex_ys.append(-1)\n",
    "        bounding_confidences.append(-1)\n",
    "        bounding_importance_fracs.append(-1)\n",
    "        dominant_blues.append(-1)\n",
    "        dominant_greens.append(-1)\n",
    "        dominant_reds.append(-1)\n",
    "        dominant_pixel_fracs.append(-1)\n",
    "        dominant_scores.append(-1)\n",
    "        label_descriptions.append('nothing')\n",
    "        label_scores.append(-1)\n",
    "        pass\n",
    "\n",
    "print(nf_count)\n",
    "test.loc[:, 'vertex_x'] = vertex_xs\n",
    "test.loc[:, 'vertex_y'] = vertex_ys\n",
    "test.loc[:, 'bounding_confidence'] = bounding_confidences\n",
    "test.loc[:, 'bounding_importance'] = bounding_importance_fracs\n",
    "test.loc[:, 'dominant_blue'] = dominant_blues\n",
    "test.loc[:, 'dominant_green'] = dominant_greens\n",
    "test.loc[:, 'dominant_red'] = dominant_reds\n",
    "test.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "test.loc[:, 'dominant_score'] = dominant_scores\n",
    "test.loc[:, 'label_description'] = label_descriptions\n",
    "test.loc[:, 'label_score'] = label_scores\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['Name', 'RescuerID', 'Description'], axis=1, inplace=True)\n",
    "test.drop(['Name', 'RescuerID', 'Description'], axis=1, inplace=True)\n",
    "\n",
    "train.drop(['dominant_green', 'doc_sent_score', 'FurLength', 'Vaccinated', \n",
    "               'Color3', 'Dewormed', 'Health', 'VideoAmt', 'Type',\n",
    "           'bounding_importance', 'bounding_confidence', 'pic_79',\n",
    "           'pic_252', 'pic_109', 'pic_197', 'pic_17', 'pic_104', 'pic_59', 'label_description'], axis=1, inplace=True)\n",
    "test.drop(['dominant_green', 'doc_sent_score', 'FurLength', 'Vaccinated', \n",
    "               'Color3', 'Dewormed', 'Health', 'VideoAmt', 'Type',\n",
    "           'bounding_importance', 'bounding_confidence', 'pic_79',\n",
    "           'pic_252', 'pic_109', 'pic_197', 'pic_17', 'pic_104', 'pic_59', 'label_description'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14993, 469)\n",
      "(3948, 469)\n",
      "saved train csv image, metadata, sentiment merged\n",
      "saved test csv image, metadata, sentiment merged\n",
      "getting categorical features\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "numeric_cols = ['Age', 'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt', 'AdoptionSpeed', \n",
    "                'doc_sent_mag', 'doc_sent_score', 'dominant_score', 'dominant_pixel_frac', \n",
    "                'dominant_red', 'dominant_green', 'dominant_blue', 'bounding_importance', \n",
    "                'bounding_confidence', 'vertex_x', 'vertex_y', 'label_score'] +\\\n",
    "               [col for col in train.columns if col.startswith('pic') or col.startswith('svd')]\n",
    "cat_cols = list(set(train.columns) - set(numeric_cols))\n",
    "train.loc[:, cat_cols] = train[cat_cols].astype('category')\n",
    "test.loc[:, cat_cols] = test[cat_cols].astype('category')\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "#print(test.head())\n",
    "drop = \"_drop100\"\n",
    "train.to_csv(r'csv_out/train_ims_merged' + drop + '.csv')\n",
    "print(\"saved train csv image, metadata, sentiment merged\")\n",
    "test.to_csv(r'csv_out/test_ims_merged' + drop +'.csv')\n",
    "print(\"saved test csv image, metadata, sentiment merged\")\n",
    "print(\"getting categorical features\")\n",
    "# get the categorical features\n",
    "foo = train.dtypes\n",
    "cat_feature_names = foo[foo == \"category\"]\n",
    "cat_features = [train.columns.get_loc(c) for c in train.columns if c in cat_feature_names]\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run LightGBM Train Model\n",
      "Started lgb fold 1/5\n",
      "Prep LGB\n",
      "Train LGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is [1, 2, 3, 4, 5, 6, 7, 10]\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "C:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 600 rounds.\n",
      "[100]\ttraining's rmse: 1.03235\tvalid_1's rmse: 1.09868\n",
      "[200]\ttraining's rmse: 0.952544\tvalid_1's rmse: 1.0737\n",
      "[300]\ttraining's rmse: 0.899835\tvalid_1's rmse: 1.06287\n",
      "[400]\ttraining's rmse: 0.857587\tvalid_1's rmse: 1.05493\n",
      "[500]\ttraining's rmse: 0.823451\tvalid_1's rmse: 1.05095\n",
      "[600]\ttraining's rmse: 0.792756\tvalid_1's rmse: 1.04751\n",
      "[700]\ttraining's rmse: 0.762964\tvalid_1's rmse: 1.0448\n",
      "[800]\ttraining's rmse: 0.735741\tvalid_1's rmse: 1.04234\n",
      "[900]\ttraining's rmse: 0.710188\tvalid_1's rmse: 1.04063\n",
      "[1000]\ttraining's rmse: 0.686433\tvalid_1's rmse: 1.0389\n",
      "[1100]\ttraining's rmse: 0.663823\tvalid_1's rmse: 1.03742\n",
      "[1200]\ttraining's rmse: 0.641759\tvalid_1's rmse: 1.03611\n",
      "[1300]\ttraining's rmse: 0.619569\tvalid_1's rmse: 1.0351\n",
      "[1400]\ttraining's rmse: 0.600611\tvalid_1's rmse: 1.03487\n",
      "[1500]\ttraining's rmse: 0.580763\tvalid_1's rmse: 1.03434\n",
      "[1600]\ttraining's rmse: 0.563629\tvalid_1's rmse: 1.03386\n",
      "[1700]\ttraining's rmse: 0.54628\tvalid_1's rmse: 1.03385\n",
      "[1800]\ttraining's rmse: 0.528688\tvalid_1's rmse: 1.03318\n",
      "[1900]\ttraining's rmse: 0.511904\tvalid_1's rmse: 1.0326\n",
      "[2000]\ttraining's rmse: 0.495768\tvalid_1's rmse: 1.03199\n",
      "[2100]\ttraining's rmse: 0.480858\tvalid_1's rmse: 1.0318\n",
      "[2200]\ttraining's rmse: 0.467164\tvalid_1's rmse: 1.03174\n",
      "[2300]\ttraining's rmse: 0.453952\tvalid_1's rmse: 1.03172\n",
      "[2400]\ttraining's rmse: 0.440832\tvalid_1's rmse: 1.03131\n",
      "[2500]\ttraining's rmse: 0.427812\tvalid_1's rmse: 1.03119\n",
      "[2600]\ttraining's rmse: 0.414285\tvalid_1's rmse: 1.03089\n",
      "[2700]\ttraining's rmse: 0.402362\tvalid_1's rmse: 1.03088\n",
      "[2800]\ttraining's rmse: 0.389312\tvalid_1's rmse: 1.03097\n",
      "[2900]\ttraining's rmse: 0.376857\tvalid_1's rmse: 1.03072\n",
      "[3000]\ttraining's rmse: 0.365179\tvalid_1's rmse: 1.03065\n",
      "[3100]\ttraining's rmse: 0.35386\tvalid_1's rmse: 1.03069\n",
      "[3200]\ttraining's rmse: 0.342678\tvalid_1's rmse: 1.03079\n",
      "[3300]\ttraining's rmse: 0.332043\tvalid_1's rmse: 1.03074\n",
      "[3400]\ttraining's rmse: 0.32232\tvalid_1's rmse: 1.03069\n",
      "[3500]\ttraining's rmse: 0.312335\tvalid_1's rmse: 1.0306\n",
      "[3600]\ttraining's rmse: 0.303058\tvalid_1's rmse: 1.03055\n",
      "[3700]\ttraining's rmse: 0.293896\tvalid_1's rmse: 1.03043\n",
      "[3800]\ttraining's rmse: 0.285155\tvalid_1's rmse: 1.03032\n",
      "[3900]\ttraining's rmse: 0.27656\tvalid_1's rmse: 1.03026\n",
      "[4000]\ttraining's rmse: 0.268514\tvalid_1's rmse: 1.03\n",
      "[4100]\ttraining's rmse: 0.259814\tvalid_1's rmse: 1.03002\n",
      "[4200]\ttraining's rmse: 0.252385\tvalid_1's rmse: 1.02994\n",
      "[4300]\ttraining's rmse: 0.245312\tvalid_1's rmse: 1.02994\n",
      "[4400]\ttraining's rmse: 0.237747\tvalid_1's rmse: 1.02989\n",
      "[4500]\ttraining's rmse: 0.230931\tvalid_1's rmse: 1.02979\n",
      "[4600]\ttraining's rmse: 0.224375\tvalid_1's rmse: 1.02982\n",
      "[4700]\ttraining's rmse: 0.217494\tvalid_1's rmse: 1.02966\n",
      "[4800]\ttraining's rmse: 0.210824\tvalid_1's rmse: 1.02967\n",
      "[4900]\ttraining's rmse: 0.203608\tvalid_1's rmse: 1.02955\n",
      "[5000]\ttraining's rmse: 0.198064\tvalid_1's rmse: 1.02958\n",
      "[5100]\ttraining's rmse: 0.192352\tvalid_1's rmse: 1.02961\n",
      "[5200]\ttraining's rmse: 0.186948\tvalid_1's rmse: 1.02962\n",
      "[5300]\ttraining's rmse: 0.181888\tvalid_1's rmse: 1.0296\n",
      "[5400]\ttraining's rmse: 0.176518\tvalid_1's rmse: 1.02948\n",
      "[5500]\ttraining's rmse: 0.171271\tvalid_1's rmse: 1.02951\n",
      "[5600]\ttraining's rmse: 0.166806\tvalid_1's rmse: 1.02953\n",
      "[5700]\ttraining's rmse: 0.162155\tvalid_1's rmse: 1.02956\n",
      "[5800]\ttraining's rmse: 0.157754\tvalid_1's rmse: 1.02951\n",
      "[5900]\ttraining's rmse: 0.153364\tvalid_1's rmse: 1.0295\n",
      "[6000]\ttraining's rmse: 0.149137\tvalid_1's rmse: 1.02943\n",
      "[6100]\ttraining's rmse: 0.145078\tvalid_1's rmse: 1.02945\n",
      "[6200]\ttraining's rmse: 0.141106\tvalid_1's rmse: 1.02936\n",
      "[6300]\ttraining's rmse: 0.137465\tvalid_1's rmse: 1.02932\n",
      "[6400]\ttraining's rmse: 0.134103\tvalid_1's rmse: 1.02924\n",
      "[6500]\ttraining's rmse: 0.131013\tvalid_1's rmse: 1.02916\n",
      "[6600]\ttraining's rmse: 0.127989\tvalid_1's rmse: 1.02909\n",
      "[6700]\ttraining's rmse: 0.125818\tvalid_1's rmse: 1.02908\n",
      "[6800]\ttraining's rmse: 0.12545\tvalid_1's rmse: 1.02906\n",
      "[6900]\ttraining's rmse: 0.12545\tvalid_1's rmse: 1.02906\n",
      "[7000]\ttraining's rmse: 0.12545\tvalid_1's rmse: 1.02906\n",
      "[7100]\ttraining's rmse: 0.12545\tvalid_1's rmse: 1.02906\n",
      "[7200]\ttraining's rmse: 0.12545\tvalid_1's rmse: 1.02906\n",
      "[7300]\ttraining's rmse: 0.12545\tvalid_1's rmse: 1.02906\n",
      "Early stopping, best iteration is:\n",
      "[6718]\ttraining's rmse: 0.125555\tvalid_1's rmse: 1.02905\n",
      "Predict 1/2\n",
      "Valid Counts =  Counter({4: 840, 2: 808, 3: 652, 1: 618, 0: 82})\n",
      "Predicted Counts =  Counter({2.0: 1226, 4.0: 822, 3.0: 619, 1.0: 333})\n",
      "Coefficients =  [0.47796796 1.84094162 2.53146991 2.87700409]\n",
      "QWK =  0.4477812225255655\n",
      "Predict 2/2\n",
      "lgb cv score 1: RMSE 1.0290491675438007 QWK 0.4477812225255655\n",
      "Started lgb fold 2/5\n",
      "Prep LGB\n",
      "Train LGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is [1, 2, 3, 4, 5, 6, 7, 10]\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "C:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 600 rounds.\n",
      "[100]\ttraining's rmse: 1.03483\tvalid_1's rmse: 1.09172\n",
      "[200]\ttraining's rmse: 0.956515\tvalid_1's rmse: 1.06467\n",
      "[300]\ttraining's rmse: 0.901748\tvalid_1's rmse: 1.05379\n",
      "[400]\ttraining's rmse: 0.857619\tvalid_1's rmse: 1.04649\n",
      "[500]\ttraining's rmse: 0.822043\tvalid_1's rmse: 1.04239\n",
      "[600]\ttraining's rmse: 0.791197\tvalid_1's rmse: 1.03939\n",
      "[700]\ttraining's rmse: 0.761967\tvalid_1's rmse: 1.03678\n",
      "[800]\ttraining's rmse: 0.735234\tvalid_1's rmse: 1.03504\n",
      "[900]\ttraining's rmse: 0.709132\tvalid_1's rmse: 1.03386\n",
      "[1000]\ttraining's rmse: 0.685215\tvalid_1's rmse: 1.03268\n",
      "[1100]\ttraining's rmse: 0.662362\tvalid_1's rmse: 1.03207\n",
      "[1200]\ttraining's rmse: 0.641286\tvalid_1's rmse: 1.03143\n",
      "[1300]\ttraining's rmse: 0.620623\tvalid_1's rmse: 1.03122\n",
      "[1400]\ttraining's rmse: 0.600039\tvalid_1's rmse: 1.03103\n",
      "[1500]\ttraining's rmse: 0.580369\tvalid_1's rmse: 1.03088\n",
      "[1600]\ttraining's rmse: 0.561275\tvalid_1's rmse: 1.03026\n",
      "[1700]\ttraining's rmse: 0.543777\tvalid_1's rmse: 1.03021\n",
      "[1800]\ttraining's rmse: 0.526447\tvalid_1's rmse: 1.02984\n",
      "[1900]\ttraining's rmse: 0.509934\tvalid_1's rmse: 1.02954\n",
      "[2000]\ttraining's rmse: 0.494174\tvalid_1's rmse: 1.02947\n",
      "[2100]\ttraining's rmse: 0.478733\tvalid_1's rmse: 1.0295\n",
      "[2200]\ttraining's rmse: 0.464197\tvalid_1's rmse: 1.02953\n",
      "[2300]\ttraining's rmse: 0.449165\tvalid_1's rmse: 1.0292\n",
      "[2400]\ttraining's rmse: 0.435512\tvalid_1's rmse: 1.02897\n",
      "[2500]\ttraining's rmse: 0.421482\tvalid_1's rmse: 1.02915\n",
      "[2600]\ttraining's rmse: 0.408726\tvalid_1's rmse: 1.02908\n",
      "[2700]\ttraining's rmse: 0.395734\tvalid_1's rmse: 1.02913\n",
      "[2800]\ttraining's rmse: 0.383762\tvalid_1's rmse: 1.02912\n",
      "[2900]\ttraining's rmse: 0.371996\tvalid_1's rmse: 1.02911\n",
      "Early stopping, best iteration is:\n",
      "[2399]\ttraining's rmse: 0.435597\tvalid_1's rmse: 1.02897\n",
      "Predict 1/2\n",
      "Valid Counts =  Counter({4: 840, 2: 808, 3: 652, 1: 618, 0: 82})\n",
      "Predicted Counts =  Counter({3.0: 934, 2.0: 770, 1.0: 690, 4.0: 606})\n",
      "Coefficients =  [0.53133793 2.07400452 2.47161625 3.01896031]\n",
      "QWK =  0.4662377135138017\n",
      "Predict 2/2\n",
      "lgb cv score 2: RMSE 1.0289742051453883 QWK 0.4662377135138017\n",
      "Started lgb fold 3/5\n",
      "Prep LGB\n",
      "Train LGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is [1, 2, 3, 4, 5, 6, 7, 10]\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "C:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 600 rounds.\n",
      "[100]\ttraining's rmse: 1.03607\tvalid_1's rmse: 1.08935\n",
      "[200]\ttraining's rmse: 0.957575\tvalid_1's rmse: 1.06086\n",
      "[300]\ttraining's rmse: 0.90204\tvalid_1's rmse: 1.04712\n",
      "[400]\ttraining's rmse: 0.8592\tvalid_1's rmse: 1.03962\n",
      "[500]\ttraining's rmse: 0.825962\tvalid_1's rmse: 1.03505\n",
      "[600]\ttraining's rmse: 0.792367\tvalid_1's rmse: 1.03146\n",
      "[700]\ttraining's rmse: 0.762464\tvalid_1's rmse: 1.02818\n",
      "[800]\ttraining's rmse: 0.735516\tvalid_1's rmse: 1.0261\n",
      "[900]\ttraining's rmse: 0.709537\tvalid_1's rmse: 1.0243\n",
      "[1000]\ttraining's rmse: 0.685496\tvalid_1's rmse: 1.02318\n",
      "[1100]\ttraining's rmse: 0.661879\tvalid_1's rmse: 1.02268\n",
      "[1200]\ttraining's rmse: 0.639904\tvalid_1's rmse: 1.0213\n",
      "[1300]\ttraining's rmse: 0.619636\tvalid_1's rmse: 1.02042\n",
      "[1400]\ttraining's rmse: 0.598963\tvalid_1's rmse: 1.02002\n",
      "[1500]\ttraining's rmse: 0.57993\tvalid_1's rmse: 1.01954\n",
      "[1600]\ttraining's rmse: 0.559765\tvalid_1's rmse: 1.01895\n",
      "[1700]\ttraining's rmse: 0.54158\tvalid_1's rmse: 1.01827\n",
      "[1800]\ttraining's rmse: 0.524308\tvalid_1's rmse: 1.01796\n",
      "[1900]\ttraining's rmse: 0.50883\tvalid_1's rmse: 1.01787\n",
      "[2000]\ttraining's rmse: 0.493545\tvalid_1's rmse: 1.01808\n",
      "[2100]\ttraining's rmse: 0.477913\tvalid_1's rmse: 1.01792\n",
      "[2200]\ttraining's rmse: 0.463384\tvalid_1's rmse: 1.01773\n",
      "[2300]\ttraining's rmse: 0.448674\tvalid_1's rmse: 1.01809\n",
      "[2400]\ttraining's rmse: 0.434706\tvalid_1's rmse: 1.01788\n",
      "[2500]\ttraining's rmse: 0.42165\tvalid_1's rmse: 1.01792\n",
      "[2600]\ttraining's rmse: 0.408731\tvalid_1's rmse: 1.01752\n",
      "[2700]\ttraining's rmse: 0.396781\tvalid_1's rmse: 1.01704\n",
      "[2800]\ttraining's rmse: 0.384615\tvalid_1's rmse: 1.01701\n",
      "[2900]\ttraining's rmse: 0.37278\tvalid_1's rmse: 1.01705\n",
      "[3000]\ttraining's rmse: 0.361526\tvalid_1's rmse: 1.01699\n",
      "[3100]\ttraining's rmse: 0.350967\tvalid_1's rmse: 1.01696\n",
      "[3200]\ttraining's rmse: 0.340724\tvalid_1's rmse: 1.01695\n",
      "[3300]\ttraining's rmse: 0.330479\tvalid_1's rmse: 1.01686\n",
      "[3400]\ttraining's rmse: 0.320344\tvalid_1's rmse: 1.01682\n",
      "[3500]\ttraining's rmse: 0.311095\tvalid_1's rmse: 1.01685\n",
      "[3600]\ttraining's rmse: 0.302877\tvalid_1's rmse: 1.01678\n",
      "[3700]\ttraining's rmse: 0.293746\tvalid_1's rmse: 1.01663\n",
      "[3800]\ttraining's rmse: 0.285573\tvalid_1's rmse: 1.0166\n",
      "[3900]\ttraining's rmse: 0.27728\tvalid_1's rmse: 1.01669\n",
      "[4000]\ttraining's rmse: 0.268492\tvalid_1's rmse: 1.01664\n",
      "[4100]\ttraining's rmse: 0.260783\tvalid_1's rmse: 1.0165\n",
      "[4200]\ttraining's rmse: 0.2529\tvalid_1's rmse: 1.01647\n",
      "[4300]\ttraining's rmse: 0.245172\tvalid_1's rmse: 1.01648\n",
      "[4400]\ttraining's rmse: 0.237722\tvalid_1's rmse: 1.01633\n",
      "[4500]\ttraining's rmse: 0.230068\tvalid_1's rmse: 1.01624\n",
      "[4600]\ttraining's rmse: 0.223566\tvalid_1's rmse: 1.0163\n",
      "[4700]\ttraining's rmse: 0.21707\tvalid_1's rmse: 1.01615\n",
      "[4800]\ttraining's rmse: 0.210556\tvalid_1's rmse: 1.01614\n",
      "[4900]\ttraining's rmse: 0.20425\tvalid_1's rmse: 1.0162\n",
      "[5000]\ttraining's rmse: 0.198153\tvalid_1's rmse: 1.01614\n",
      "[5100]\ttraining's rmse: 0.192382\tvalid_1's rmse: 1.0162\n",
      "[5200]\ttraining's rmse: 0.187044\tvalid_1's rmse: 1.01614\n",
      "[5300]\ttraining's rmse: 0.181581\tvalid_1's rmse: 1.01614\n",
      "Early stopping, best iteration is:\n",
      "[4778]\ttraining's rmse: 0.212145\tvalid_1's rmse: 1.01606\n",
      "Predict 1/2\n",
      "Valid Counts =  Counter({4: 839, 2: 807, 3: 652, 1: 618, 0: 82})\n",
      "Predicted Counts =  Counter({3.0: 997, 2.0: 934, 4.0: 709, 1.0: 357, 0.0: 1})\n",
      "Coefficients =  [0.53157545 1.83618963 2.37751915 2.95979258]\n",
      "QWK =  0.4656069933953546\n",
      "Predict 2/2\n",
      "lgb cv score 3: RMSE 1.016058904395322 QWK 0.4656069933953546\n",
      "Started lgb fold 4/5\n",
      "Prep LGB\n",
      "Train LGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is [1, 2, 3, 4, 5, 6, 7, 10]\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "C:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 600 rounds.\n",
      "[100]\ttraining's rmse: 1.03605\tvalid_1's rmse: 1.0941\n",
      "[200]\ttraining's rmse: 0.956812\tvalid_1's rmse: 1.06617\n",
      "[300]\ttraining's rmse: 0.902054\tvalid_1's rmse: 1.05293\n",
      "[400]\ttraining's rmse: 0.858114\tvalid_1's rmse: 1.04568\n",
      "[500]\ttraining's rmse: 0.821081\tvalid_1's rmse: 1.04112\n",
      "[600]\ttraining's rmse: 0.7897\tvalid_1's rmse: 1.03821\n",
      "[700]\ttraining's rmse: 0.760117\tvalid_1's rmse: 1.0362\n",
      "[800]\ttraining's rmse: 0.732849\tvalid_1's rmse: 1.03467\n",
      "[900]\ttraining's rmse: 0.708229\tvalid_1's rmse: 1.03361\n",
      "[1000]\ttraining's rmse: 0.684844\tvalid_1's rmse: 1.0331\n",
      "[1100]\ttraining's rmse: 0.662493\tvalid_1's rmse: 1.03257\n",
      "[1200]\ttraining's rmse: 0.642519\tvalid_1's rmse: 1.03187\n",
      "[1300]\ttraining's rmse: 0.621951\tvalid_1's rmse: 1.03107\n",
      "[1400]\ttraining's rmse: 0.602524\tvalid_1's rmse: 1.03072\n",
      "[1500]\ttraining's rmse: 0.584593\tvalid_1's rmse: 1.03073\n",
      "[1600]\ttraining's rmse: 0.566493\tvalid_1's rmse: 1.03074\n",
      "[1700]\ttraining's rmse: 0.548328\tvalid_1's rmse: 1.03048\n",
      "[1800]\ttraining's rmse: 0.531213\tvalid_1's rmse: 1.03021\n",
      "[1900]\ttraining's rmse: 0.515311\tvalid_1's rmse: 1.03026\n",
      "[2000]\ttraining's rmse: 0.499645\tvalid_1's rmse: 1.03011\n",
      "[2100]\ttraining's rmse: 0.484202\tvalid_1's rmse: 1.03016\n",
      "[2200]\ttraining's rmse: 0.469089\tvalid_1's rmse: 1.03018\n",
      "[2300]\ttraining's rmse: 0.453993\tvalid_1's rmse: 1.03039\n",
      "[2400]\ttraining's rmse: 0.440751\tvalid_1's rmse: 1.03044\n",
      "[2500]\ttraining's rmse: 0.42639\tvalid_1's rmse: 1.03064\n",
      "[2600]\ttraining's rmse: 0.413061\tvalid_1's rmse: 1.0306\n",
      "[2700]\ttraining's rmse: 0.401349\tvalid_1's rmse: 1.03077\n",
      "Early stopping, best iteration is:\n",
      "[2174]\ttraining's rmse: 0.472928\tvalid_1's rmse: 1.03001\n",
      "Predict 1/2\n",
      "Valid Counts =  Counter({4: 839, 2: 807, 3: 652, 1: 618, 0: 82})\n",
      "Predicted Counts =  Counter({2.0: 1257, 4.0: 794, 3.0: 638, 1.0: 309})\n",
      "Coefficients =  [0.51248436 1.80666855 2.49816257 2.87886551]\n",
      "QWK =  0.45740206207272993\n",
      "Predict 2/2\n",
      "lgb cv score 4: RMSE 1.030007080981278 QWK 0.45740206207272993\n",
      "Started lgb fold 5/5\n",
      "Prep LGB\n",
      "Train LGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is [1, 2, 3, 4, 5, 6, 7, 10]\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "C:\\Users\\Sunfl\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 600 rounds.\n",
      "[100]\ttraining's rmse: 1.03502\tvalid_1's rmse: 1.09558\n",
      "[200]\ttraining's rmse: 0.956502\tvalid_1's rmse: 1.06696\n",
      "[300]\ttraining's rmse: 0.901047\tvalid_1's rmse: 1.05397\n",
      "[400]\ttraining's rmse: 0.855031\tvalid_1's rmse: 1.04721\n",
      "[500]\ttraining's rmse: 0.815265\tvalid_1's rmse: 1.04279\n",
      "[600]\ttraining's rmse: 0.782181\tvalid_1's rmse: 1.04035\n",
      "[700]\ttraining's rmse: 0.753528\tvalid_1's rmse: 1.03862\n",
      "[800]\ttraining's rmse: 0.72784\tvalid_1's rmse: 1.03698\n",
      "[900]\ttraining's rmse: 0.701448\tvalid_1's rmse: 1.0357\n",
      "[1000]\ttraining's rmse: 0.677741\tvalid_1's rmse: 1.03512\n",
      "[1100]\ttraining's rmse: 0.654278\tvalid_1's rmse: 1.03457\n",
      "[1200]\ttraining's rmse: 0.63132\tvalid_1's rmse: 1.03399\n",
      "[1300]\ttraining's rmse: 0.61104\tvalid_1's rmse: 1.03308\n",
      "[1400]\ttraining's rmse: 0.591977\tvalid_1's rmse: 1.03288\n",
      "[1500]\ttraining's rmse: 0.572553\tvalid_1's rmse: 1.03238\n",
      "[1600]\ttraining's rmse: 0.554933\tvalid_1's rmse: 1.03222\n",
      "[1700]\ttraining's rmse: 0.537442\tvalid_1's rmse: 1.03203\n",
      "[1800]\ttraining's rmse: 0.520614\tvalid_1's rmse: 1.03176\n",
      "[1900]\ttraining's rmse: 0.505624\tvalid_1's rmse: 1.03147\n",
      "[2000]\ttraining's rmse: 0.490591\tvalid_1's rmse: 1.03164\n",
      "[2100]\ttraining's rmse: 0.475235\tvalid_1's rmse: 1.03183\n",
      "[2200]\ttraining's rmse: 0.460582\tvalid_1's rmse: 1.03185\n",
      "[2300]\ttraining's rmse: 0.445838\tvalid_1's rmse: 1.03188\n",
      "[2400]\ttraining's rmse: 0.432694\tvalid_1's rmse: 1.03186\n",
      "[2500]\ttraining's rmse: 0.419871\tvalid_1's rmse: 1.0321\n",
      "Early stopping, best iteration is:\n",
      "[1914]\ttraining's rmse: 0.503577\tvalid_1's rmse: 1.03141\n",
      "Predict 1/2\n",
      "Valid Counts =  Counter({4: 839, 2: 807, 3: 651, 1: 618, 0: 82})\n",
      "Predicted Counts =  Counter({2.0: 1006, 4.0: 845, 1.0: 661, 3.0: 485})\n",
      "Coefficients =  [0.48105218 2.06112682 2.54715681 2.80594453]\n",
      "QWK =  0.46500746683340455\n",
      "Predict 2/2\n",
      "lgb cv score 5: RMSE 1.0314092813335254 QWK 0.46500746683340455\n",
      "lgb cv RMSE scores : [1.0290491675438007, 1.0289742051453883, 1.016058904395322, 1.030007080981278, 1.0314092813335254]\n",
      "lgb cv mean RMSE score : 1.027099727879863\n",
      "lgb cv std RMSE score : 1.027099727879863\n",
      "lgb cv QWK scores : [0.4477812225255655, 0.4662377135138017, 0.4656069933953546, 0.45740206207272993, 0.46500746683340455]\n",
      "lgb cv mean QWK score : 0.4604070916681713\n",
      "lgb cv std QWK score : 0.00708013213458174\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "N_SPLITS = 5\n",
    "def run_cv_model(train, test, target, model_fn, params={}, eval_fn=None, label='model'):\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, random_state=42, shuffle=True)\n",
    "    fold_splits = kf.split(train, target)\n",
    "    cv_scores = []\n",
    "    qwk_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros((train.shape[0], N_SPLITS))\n",
    "    all_coefficients = np.zeros((N_SPLITS, 4))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    i = 1\n",
    "    for dev_index, val_index in fold_splits:\n",
    "        print('Started ' + label + ' fold ' + str(i) + '/' + str(N_SPLITS))\n",
    "        if isinstance(train, pd.DataFrame):\n",
    "            dev_X, val_X = train.iloc[dev_index], train.iloc[val_index]\n",
    "            dev_y, val_y = target[dev_index], target[val_index]\n",
    "        else:\n",
    "            dev_X, val_X = train[dev_index], train[val_index]\n",
    "            dev_y, val_y = target[dev_index], target[val_index]\n",
    "        params2 = params.copy()\n",
    "        pred_val_y, pred_test_y, importances, coefficients, qwk = model_fn(dev_X, dev_y, val_X, val_y, test, params2)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index] = pred_val_y\n",
    "        all_coefficients[i-1, :] = coefficients\n",
    "        if eval_fn is not None:\n",
    "            cv_score = eval_fn(val_y, pred_val_y)\n",
    "            cv_scores.append(cv_score)\n",
    "            qwk_scores.append(qwk)\n",
    "            print(label + ' cv score {}: RMSE {} QWK {}'.format(i, cv_score, qwk))\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature'] = train.columns.values\n",
    "        fold_importance_df['importance'] = importances\n",
    "        fold_importance_df['fold'] = i\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)        \n",
    "        i += 1\n",
    "    print('{} cv RMSE scores : {}'.format(label, cv_scores))\n",
    "    print('{} cv mean RMSE score : {}'.format(label, np.mean(cv_scores)))\n",
    "    print('{} cv std RMSE score : {}'.format(label, np.mean(cv_scores)))\n",
    "    print('{} cv QWK scores : {}'.format(label, qwk_scores))\n",
    "    print('{} cv mean QWK score : {}'.format(label, np.mean(qwk_scores)))\n",
    "    print('{} cv std QWK score : {}'.format(label, np.std(qwk_scores)))\n",
    "    pred_full_test = pred_full_test / float(N_SPLITS)\n",
    "    results = {'label': label,\n",
    "               'train': pred_train, 'test': pred_full_test,\n",
    "                'cv': cv_scores, 'qwk': qwk_scores,\n",
    "               'importance': feature_importance_df,\n",
    "               'coefficients': all_coefficients}\n",
    "    return results\n",
    "\n",
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 70,\n",
    "          'max_depth': 9,\n",
    "          'learning_rate': 0.01,\n",
    "          'bagging_fraction': 0.85,\n",
    "          'feature_fraction': 0.8,\n",
    "          'min_split_gain': 0.02,\n",
    "          'min_child_samples': 150,\n",
    "          'min_child_weight': 0.02,\n",
    "          'lambda_l2': 0.0475,\n",
    "          'verbosity': -1,\n",
    "          'data_random_seed': 17,\n",
    "          'early_stop': 600,\n",
    "          'verbose_eval': 100,\n",
    "          'num_rounds': 10000}\n",
    "\n",
    "def runLGB(train_X, train_y, test_X, test_y, test_X2, params):\n",
    "    print('Prep LGB')\n",
    "    d_train = lgb.Dataset(train_X, label=train_y)\n",
    "    d_valid = lgb.Dataset(test_X, label=test_y)\n",
    "    watchlist = [d_train, d_valid]\n",
    "    print('Train LGB')\n",
    "    num_rounds = params.pop('num_rounds')\n",
    "    verbose_eval = params.pop('verbose_eval')\n",
    "    early_stop = None\n",
    "    if params.get('early_stop'):\n",
    "        early_stop = params.pop('early_stop')\n",
    "    model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=num_rounds,\n",
    "                      valid_sets=watchlist,\n",
    "                      verbose_eval=verbose_eval,\n",
    "                      categorical_feature=list(cat_features),\n",
    "                      early_stopping_rounds=early_stop)\n",
    "    \n",
    "    print('Predict 1/2')\n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    optR = OptimizedRounder()\n",
    "    optR.fit(pred_test_y, test_y)\n",
    "    coefficients = optR.coefficients()\n",
    "    pred_test_y_k = optR.predict(pred_test_y, coefficients)\n",
    "    print(\"Valid Counts = \", Counter(test_y))\n",
    "    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "    print(\"Coefficients = \", coefficients)\n",
    "    qwk = quadratic_weighted_kappa(test_y, pred_test_y_k)\n",
    "    print(\"QWK = \", qwk)\n",
    "    print('Predict 2/2')\n",
    "    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n",
    "    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), model.feature_importance(), coefficients, qwk\n",
    "\n",
    "print(\"Run LightGBM Train Model\")\n",
    "results = run_cv_model(train, test, target, runLGB, params, rmse, 'lgb')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breed1</td>\n",
       "      <td>2046.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Age</td>\n",
       "      <td>792.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Breed2</td>\n",
       "      <td>598.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>State</td>\n",
       "      <td>466.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>svd_153</td>\n",
       "      <td>431.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>svd_47</td>\n",
       "      <td>430.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>svd_45</td>\n",
       "      <td>417.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>pic_224</td>\n",
       "      <td>401.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>svd_66</td>\n",
       "      <td>398.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>svd_70</td>\n",
       "      <td>392.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>label_score</td>\n",
       "      <td>390.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>svd_52</td>\n",
       "      <td>387.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>svd_0</td>\n",
       "      <td>386.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>pic_8</td>\n",
       "      <td>385.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>svd_157</td>\n",
       "      <td>382.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>svd_175</td>\n",
       "      <td>378.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>svd_30</td>\n",
       "      <td>377.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>svd_199</td>\n",
       "      <td>377.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>svd_178</td>\n",
       "      <td>373.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>svd_75</td>\n",
       "      <td>373.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>svd_141</td>\n",
       "      <td>372.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>svd_61</td>\n",
       "      <td>370.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>pic_189</td>\n",
       "      <td>369.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>pic_49</td>\n",
       "      <td>367.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>svd_169</td>\n",
       "      <td>364.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>svd_140</td>\n",
       "      <td>364.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>pic_160</td>\n",
       "      <td>364.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>svd_83</td>\n",
       "      <td>358.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Quantity</td>\n",
       "      <td>358.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>svd_134</td>\n",
       "      <td>355.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>pic_215</td>\n",
       "      <td>196.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>pic_20</td>\n",
       "      <td>194.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>pic_27</td>\n",
       "      <td>194.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>pic_241</td>\n",
       "      <td>193.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>pic_60</td>\n",
       "      <td>191.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>pic_140</td>\n",
       "      <td>191.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>pic_173</td>\n",
       "      <td>187.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>pic_170</td>\n",
       "      <td>186.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>pic_22</td>\n",
       "      <td>185.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>pic_143</td>\n",
       "      <td>185.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>pic_4</td>\n",
       "      <td>184.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>pic_10</td>\n",
       "      <td>184.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dominant_red</td>\n",
       "      <td>183.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>pic_93</td>\n",
       "      <td>183.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>pic_90</td>\n",
       "      <td>182.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>pic_34</td>\n",
       "      <td>179.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>pic_114</td>\n",
       "      <td>178.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>pic_119</td>\n",
       "      <td>178.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>pic_62</td>\n",
       "      <td>178.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>pic_222</td>\n",
       "      <td>175.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>pic_88</td>\n",
       "      <td>173.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sterilized</td>\n",
       "      <td>173.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>svd_167</td>\n",
       "      <td>172.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>pic_5</td>\n",
       "      <td>168.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gender</td>\n",
       "      <td>168.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>pic_26</td>\n",
       "      <td>167.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>pic_43</td>\n",
       "      <td>161.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>pic_100</td>\n",
       "      <td>157.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>pic_38</td>\n",
       "      <td>151.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MaturitySize</td>\n",
       "      <td>139.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>469 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature  importance\n",
       "1          Breed1      2046.6\n",
       "0             Age       792.2\n",
       "2          Breed2       598.0\n",
       "10          State       466.8\n",
       "328       svd_153       431.4\n",
       "409        svd_47       430.6\n",
       "407        svd_45       417.8\n",
       "154       pic_224       401.0\n",
       "430        svd_66       398.2\n",
       "435        svd_70       392.2\n",
       "17    label_score       390.4\n",
       "415        svd_52       387.8\n",
       "267         svd_0       386.8\n",
       "245         pic_8       385.0\n",
       "332       svd_157       382.6\n",
       "352       svd_175       378.0\n",
       "391        svd_30       377.4\n",
       "378       svd_199       377.0\n",
       "355       svd_178       373.8\n",
       "440        svd_75       373.2\n",
       "315       svd_141       372.2\n",
       "425        svd_61       370.4\n",
       "115       pic_189       369.2\n",
       "213        pic_49       367.0\n",
       "345       svd_169       364.8\n",
       "314       svd_140       364.4\n",
       "85        pic_160       364.0\n",
       "449        svd_83       358.8\n",
       "9        Quantity       358.8\n",
       "307       svd_134       355.4\n",
       "..            ...         ...\n",
       "144       pic_215       196.4\n",
       "127        pic_20       194.4\n",
       "189        pic_27       194.2\n",
       "173       pic_241       193.2\n",
       "225        pic_60       191.4\n",
       "63        pic_140       191.0\n",
       "98        pic_173       187.8\n",
       "95        pic_170       186.8\n",
       "149        pic_22       185.6\n",
       "66        pic_143       185.4\n",
       "203         pic_4       184.4\n",
       "20         pic_10       184.4\n",
       "15   dominant_red       183.8\n",
       "260        pic_93       183.4\n",
       "257        pic_90       182.8\n",
       "197        pic_34       179.6\n",
       "34        pic_114       178.8\n",
       "39        pic_119       178.2\n",
       "227        pic_62       178.2\n",
       "152       pic_222       175.8\n",
       "254        pic_88       173.6\n",
       "11     Sterilized       173.0\n",
       "343       svd_167       172.8\n",
       "214         pic_5       168.8\n",
       "6          Gender       168.6\n",
       "188        pic_26       167.2\n",
       "207        pic_43       161.4\n",
       "21        pic_100       157.8\n",
       "201        pic_38       151.4\n",
       "7    MaturitySize       139.2\n",
       "\n",
       "[469 rows x 2 columns]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imports = results['importance'].groupby('feature')['feature', 'importance'].mean().reset_index()\n",
    "imports.sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving out feature importance list to CSV\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"saving out feature importance list to CSV\")\n",
    "imports.to_csv(\"csv_out/features_importance.csv\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50688358 1.92378623 2.48518494 2.90811341]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({2: 3689, 1: 3079, 3: 3184, 4: 4282, 0: 759})"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optR = OptimizedRounder()\n",
    "coefficients_ = np.mean(results['coefficients'], axis=0)\n",
    "print(coefficients_)\n",
    "# manually adjust coefs\n",
    "coefficients_[0] = 1.645\n",
    "coefficients_[1] = 2.115\n",
    "coefficients_[3] = 2.84\n",
    "train_predictions = [r[0] for r in results['train']]\n",
    "train_predictions = optR.predict(train_predictions, coefficients_).astype(int)\n",
    "Counter(train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50688358 1.92378623 2.48518494 2.90811341]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({2: 998, 4: 1159, 3: 857, 1: 792, 0: 142})"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optR = OptimizedRounder()\n",
    "coefficients_ = np.mean(results['coefficients'], axis=0)\n",
    "print(coefficients_)\n",
    "# manually adjust coefs\n",
    "coefficients_[0] = 1.645\n",
    "coefficients_[1] = 2.115\n",
    "coefficients_[3] = 2.84\n",
    "test_predictions = [r[0] for r in results['test']]\n",
    "test_predictions = optR.predict(test_predictions, coefficients_).astype(int)\n",
    "Counter(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Distribution:\n",
      "0    0.027346\n",
      "1    0.206096\n",
      "2    0.269259\n",
      "3    0.217368\n",
      "4    0.279931\n",
      "Name: AdoptionSpeed, dtype: float64\n",
      "Test Predicted Distribution:\n",
      "0    0.035968\n",
      "1    0.200608\n",
      "2    0.252786\n",
      "3    0.217072\n",
      "4    0.293566\n",
      "dtype: float64\n",
      "Train Predicted Distribution:\n",
      "0    0.050624\n",
      "1    0.205363\n",
      "2    0.246048\n",
      "3    0.212366\n",
      "4    0.285600\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"True Distribution:\")\n",
    "print(pd.value_counts(target, normalize=True).sort_index())\n",
    "print(\"Test Predicted Distribution:\")\n",
    "print(pd.value_counts(test_predictions, normalize=True).sort_index())\n",
    "print(\"Train Predicted Distribution:\")\n",
    "print(pd.value_counts(train_predictions, normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84</td>\n",
       "      <td>119</td>\n",
       "      <td>95</td>\n",
       "      <td>61</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>371</td>\n",
       "      <td>1097</td>\n",
       "      <td>851</td>\n",
       "      <td>491</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>1070</td>\n",
       "      <td>1199</td>\n",
       "      <td>876</td>\n",
       "      <td>692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81</td>\n",
       "      <td>578</td>\n",
       "      <td>938</td>\n",
       "      <td>862</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>215</td>\n",
       "      <td>606</td>\n",
       "      <td>894</td>\n",
       "      <td>2459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4\n",
       "0   84   119    95   61    51\n",
       "1  371  1097   851  491   280\n",
       "2  200  1070  1199  876   692\n",
       "3   81   578   938  862   800\n",
       "4   23   215   606  894  2459"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(sk_cmatrix(target, train_predictions), index=list(range(5)), columns=list(range(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PetID</th>\n",
       "      <th>AdoptionSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>378fcc4fc</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73c10e136</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72000c4c5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e147a4b9f</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43fbba852</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PetID  AdoptionSpeed\n",
       "0  378fcc4fc              2\n",
       "1  73c10e136              4\n",
       "2  72000c4c5              4\n",
       "3  e147a4b9f              4\n",
       "4  43fbba852              4"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quadratic_weighted_kappa(target, train_predictions)\n",
    "rmse(target, [r[0] for r in results['train']])\n",
    "submission = pd.DataFrame({'PetID': test_id, 'AdoptionSpeed': test_predictions})\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submissions/LightBGM with image features/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
