{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import lightgbm as lgb\n",
    "np.random.seed(369)\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following 3 functions have been taken from Ben Hamner's github repository\n",
    "# https://github.com/benhamner/Metrics\n",
    "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = y\n",
    "    rater_b = y_pred\n",
    "    min_rating=None\n",
    "    max_rating=None\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return (1.0 - numerator / denominator)\n",
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        ll = quadratic_weighted_kappa(y, X_p)\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']\n",
    "    \n",
    "def rmse(actual, predicted):\n",
    "    return sqrt(mean_squared_error(actual, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Features ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from keras.applications.densenet import preprocess_input, DenseNet121\n",
    "import tensorflow as tf\n",
    "\n",
    "train_df = pd.read_csv('input/train/train.csv')\n",
    "img_size = 256\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pet_ids = train_df['PetID'].values\n",
    "n_batches = len(pet_ids) // batch_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_to_square(im):\n",
    "    old_size = im.shape[:2] # old_size is in (height, width) format\n",
    "    ratio = float(img_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    # new_size should be in (width, height) format\n",
    "    im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "    delta_w = img_size - new_size[1]\n",
    "    delta_h = img_size - new_size[0]\n",
    "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "    color = [0, 0, 0]\n",
    "    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n",
    "    return new_im\n",
    "\n",
    "def load_image(path, pet_id):\n",
    "    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n",
    "    new_image = resize_to_square(image)\n",
    "    new_image = preprocess_input(new_image)\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "\n",
    "def train_keras():\n",
    "    from keras.models import Model\n",
    "    from keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "\n",
    "    #https://regressionsessionsblog.wordpress.com/2018/06/11/stuck-on-an-issue-making-keras-predictions-in-parallel/\n",
    "    #https://github.com/keras-team/keras/issues/4740\n",
    "    jobs = 6\n",
    "    config = tf.ConfigProto(intra_op_parallelism_threads=jobs, \\\n",
    "                            inter_op_parallelism_threads=jobs, \\\n",
    "                            allow_soft_placement=True, \\\n",
    "                            device_count = {'CPU': jobs})\n",
    "    session = tf.Session(config=config)\n",
    "    K.set_session(session)\n",
    "\n",
    "    print(\"start\")\n",
    "    inp = Input((256,256,3))\n",
    "    backbone = DenseNet121(input_tensor = inp, \n",
    "                            weights=\"input/densenet-keras/DenseNet-BC-121-32-no-top.h5\",\n",
    "                            include_top = False)\n",
    "    x = backbone.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\n",
    "    x = AveragePooling1D(4)(x)\n",
    "    out = Lambda(lambda x: x[:,:,0])(x)\n",
    "\n",
    "    m = Model(inp,out)\n",
    "    print(\"done\")\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"start keras prediction\")\n",
    "\n",
    "    for b in tqdm_notebook(range(n_batches)):\n",
    "        start = b*batch_size\n",
    "        end = (b+1)*batch_size\n",
    "        batch_pets = pet_ids[start:end]\n",
    "        batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            try:\n",
    "                batch_images[i] = load_image(\"input/train/train_images/\", pet_id)\n",
    "            except:\n",
    "                pass\n",
    "        batch_preds = m.predict(batch_images)\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            features[pet_id] = batch_preds[i]\n",
    "    print(\"done keras prediction\")\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "done\n",
      "start keras prediction\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e67e192f3af4941a9a55ca75f1ae3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=938), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done keras prediction\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "m = train_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(features, filename=\"train_features\"):\n",
    "    with open(filename + \".json\", 'w') as file:\n",
    "        json.dump(features,file)\n",
    "        #json.dump(dictionary, file, sort_keys=True, indent=4)\n",
    "    return\n",
    "def load_json(filename=\"file\", dictionary=features):\n",
    "    with open(filename + \".json\", 'r') as file:\n",
    "        loaded_file = json.load(file)\n",
    "    return loaded_file\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,\n",
    "            np.int16, np.int32, np.int64, np.uint8,\n",
    "            np.uint16, np.uint32, np.uint64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.float_, np.float16, np.float32, \n",
    "            np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj,(np.ndarray,)): #### This is the fix\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "dumped = json.dumps(features, cls=NumpyEncoder)\n",
    "\n",
    "with open(\"train_features.json\", 'w') as f:\n",
    "    json.dump(dumped, f)\n",
    "    print(\"Saved train features json to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model weights to disk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(m.summary())\n",
    "\n",
    "m.save_weights(\"keras_model_weights.h5\")\n",
    "print(\"Saved model weights to disk\")\n",
    "#with open(\"train_features.json\", 'r') as f:\n",
    "#    train_loaded_features = json.load(f)\n",
    "#    train_loaded_features = json.loads(train_loaded_features)\n",
    "#    print(\"loaded train features\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start create model\n",
      "done\n",
      "start keras test prediction\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087b36f307d34e90a643ec0bb94c3825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=247), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done keras test prediction\n",
      "Saved test features json to disk\n"
     ]
    }
   ],
   "source": [
    "train_feats = pd.DataFrame.from_dict(features, orient='index')\n",
    "train_feats.columns = ['pic_'+str(i) for i in range(train_feats.shape[1])]\n",
    "test_df = pd.read_csv('input/test/test.csv')\n",
    "\n",
    "pet_ids = test_df['PetID'].values\n",
    "n_batches = len(pet_ids) // batch_size + 1\n",
    "test_features = {}\n",
    "\n",
    "def train_test_keras(m, n_batches):\n",
    "    from keras.models import Model\n",
    "    from keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "\n",
    "    #https://regressionsessionsblog.wordpress.com/2018/06/11/stuck-on-an-issue-making-keras-predictions-in-parallel/\n",
    "    #https://github.com/keras-team/keras/issues/4740\n",
    "    jobs = 6\n",
    "    config = tf.ConfigProto(intra_op_parallelism_threads=jobs, \\\n",
    "                            inter_op_parallelism_threads=jobs, \\\n",
    "                            allow_soft_placement=True, \\\n",
    "                            device_count = {'CPU': jobs})\n",
    "    session = tf.Session(config=config)\n",
    "    K.set_session(session)\n",
    "\n",
    "    print(\"start create model\")\n",
    "    inp = Input((256,256,3))\n",
    "    backbone = DenseNet121(input_tensor = inp, \n",
    "                            weights=\"input/densenet-keras/DenseNet-BC-121-32-no-top.h5\",\n",
    "                            include_top = False)\n",
    "    x = backbone.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\n",
    "    x = AveragePooling1D(4)(x)\n",
    "    out = Lambda(lambda x: x[:,:,0])(x)\n",
    "\n",
    "    m = Model(inp,out)\n",
    "    print(\"done\")\n",
    "    \n",
    "    print(\"start keras test prediction\")\n",
    "\n",
    "    for b in tqdm_notebook(range(n_batches)):\n",
    "        start = b*batch_size\n",
    "        end = (b+1)*batch_size\n",
    "        batch_pets = pet_ids[start:end]\n",
    "        batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            try:\n",
    "                batch_images[i] = load_image(\"input/test/test_images/\", pet_id)\n",
    "            except:\n",
    "                pass\n",
    "        batch_preds = m.predict(batch_images)\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            test_features[pet_id] = batch_preds[i]\n",
    "    print(\"done keras test prediction\")\n",
    "    return\n",
    "\n",
    "train_test_keras(m, n_batches)\n",
    "\n",
    "dumped = json.dumps(test_features, cls=NumpyEncoder)\n",
    "\n",
    "with open(\"test_features.json\", 'w') as f:\n",
    "    json.dump(dumped, f)\n",
    "    print(\"Saved test features json to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved image test features prediction values to CSV\n"
     ]
    }
   ],
   "source": [
    "test_feats = pd.DataFrame.from_dict(features, orient='index')\n",
    "test_feats.columns = ['pic_'+str(i) for i in range(test_feats.shape[1])]\n",
    "test_feats = test_feats.reset_index()\n",
    "test_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n",
    "\n",
    "train_feats = train_feats.reset_index()\n",
    "train_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n",
    "\n",
    "test_feats.head()\n",
    "\n",
    "test_feats.to_csv(r'csv_out/img_test_feats_prediction_values.csv')\n",
    "print(\"saved image test features prediction values to CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded train features\n",
      "loaded test features\n",
      "saved train csv image merge\n",
      "saved test csv image merge\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"input/train/train.csv\")\n",
    "test = pd.read_csv(\"input/test/test.csv\")\n",
    "\n",
    "target = train['AdoptionSpeed']\n",
    "train_id = train['PetID']\n",
    "test_id = test['PetID']\n",
    "\n",
    "loaded_train_feats = {}\n",
    "loaded_test_feats = {}\n",
    "with open(\"train_features.json\", 'r') as f:\n",
    "    loaded_train_feats = json.load(f)\n",
    "    loaded_train_feats = json.loads(loaded_train_feats)\n",
    "    print(\"loaded train features\")\n",
    "\n",
    "with open(\"test_features.json\", 'r') as f:\n",
    "    loaded_test_feats = json.load(f)\n",
    "    loaded_test_feats = json.loads(loaded_test_feats)\n",
    "    print(\"loaded test features\") \n",
    "\n",
    "train_feats = pd.DataFrame.from_dict(loaded_train_feats, orient='index')\n",
    "train_feats.columns = ['pic_'+str(i) for i in range(train_feats.shape[1])]\n",
    "\n",
    "test_feats = pd.DataFrame.from_dict(loaded_test_feats, orient='index')\n",
    "test_feats.columns = ['pic_'+str(i) for i in range(test_feats.shape[1])]\n",
    "\n",
    "train_feats = train_feats.reset_index()\n",
    "train_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n",
    "test_feats = test_feats.reset_index()\n",
    "test_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n",
    "\n",
    "\n",
    "train = train.join(train_feats.set_index('PetID'),on='PetID')\n",
    "test = test.join(test_feats.set_index('PetID'),on='PetID')\n",
    "#train = pd.merge(train, train_feats, on = ['PetID'], how = 'left')\n",
    "#test = pd.merge(test, test_feats, left_on = ['PetID'], right_on = ['PetID'], how = 'outer')\n",
    "\n",
    "train.drop(['AdoptionSpeed', 'PetID'], axis=1, inplace=True)\n",
    "test.drop(['PetID'], axis=1, inplace=True)\n",
    "\n",
    "train.to_csv(r'csv_out/train_csv_img_merge.csv')\n",
    "print(\"saved train csv image merge\")\n",
    "train.to_csv(r'csv_out/test_csv_img_merge.csv')\n",
    "print(\"saved test csv image merge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
